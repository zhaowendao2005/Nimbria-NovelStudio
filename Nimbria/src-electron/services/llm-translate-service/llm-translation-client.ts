/**
 * LLM Translation Client
 * 
 * @description
 * ç¿»è¯‘ä¸“ç”¨å®¢æˆ·ç«¯ï¼Œè´Ÿè´£å•æ¬¡ç¿»è¯‘è¯·æ±‚çš„å‘é€å’Œæ¥æ”¶
 * 
 * èŒè´£ï¼š
 * - å•ä¾‹ç¿»è¯‘ä»»åŠ¡çš„å‘é€å’Œæ¥æ”¶
 * - æµå¼å“åº”å¤„ç†
 * - Token è®¡æ•°å’Œè´¹ç”¨ä¼°ç®—
 * - é”™è¯¯å¤„ç†å’Œé‡è¯•
 * 
 * ç‰¹ç‚¹ï¼š
 * - æ— çŠ¶æ€è®¾è®¡ï¼ˆä¸ä¿å­˜å†å²è®°å½•ï¼‰
 * - è½»é‡çº§ï¼ˆæ¯ä¸ªç¿»è¯‘ä»»åŠ¡ç‹¬ç«‹ï¼‰
 * - è‡ªåŠ¨æ¸…ç†ï¼ˆä»»åŠ¡å®Œæˆåé‡Šæ”¾èµ„æºï¼‰
 */

import { EventEmitter } from 'events'
import type { LangChainClient } from '../llm-chat-service/langchain-client'
import type {
  TranslationClientConfig,
  TranslationRequest,
  TranslationResult,
  TranslationStreamCallbacks,
  ErrorType,
  ModelConfig
} from '../../types/LlmTranslate/backend'

export class LlmTranslationClient extends EventEmitter {
  private config: TranslationClientConfig
  private llmConfigManager: any
  private activeClient: LangChainClient | null = null

  constructor(config: TranslationClientConfig, llmConfigManager: any) {
    super()
    this.config = config
    this.llmConfigManager = llmConfigManager
  }

  /**
   * æ‰§è¡Œå•æ¬¡ç¿»è¯‘ï¼ˆæµå¼ï¼‰
   */
  async translateStream(
    request: TranslationRequest,
    callbacks: TranslationStreamCallbacks
  ): Promise<TranslationResult> {
    const startTime = Date.now()
    let translation = ''
    let outputTokens = 0

    try {
      // 1. åˆå§‹åŒ– LangChain å®¢æˆ·ç«¯
      const client = await this.initClient()

      // 2. å‘å°„å¼€å§‹äº‹ä»¶
      callbacks.onStart?.(request.taskId)
      this.emit('translation:start', { 
        taskId: request.taskId,
        estimatedTokens: request.estimatedTokens 
      })

      // 3. æ„å»ºæ¶ˆæ¯
      const messages = [
        { 
          id: 'system', 
          role: 'system' as const, 
          content: this.config.systemPrompt,
          timestamp: new Date()
        },
        { 
          id: 'user', 
          role: 'user' as const, 
          content: request.content,
          timestamp: new Date()
        }
      ]

      // 4. æµå¼è°ƒç”¨ LLM
      await client.chatStream(messages, {
        onChunk: (chunk: string) => {
          translation += chunk
          outputTokens = this.estimateTokens(translation)
          
          // è¿›åº¦å›è°ƒ
          callbacks.onProgress?.(request.taskId, chunk, outputTokens)
          
          // å‘å°„è¿›åº¦äº‹ä»¶
          this.emit('translation:progress', {
            taskId: request.taskId,
            chunk,
            currentTokens: outputTokens,
            estimatedTokens: request.estimatedTokens,
            progress: Math.min((outputTokens / request.estimatedTokens) * 100, 100)
          })
        },
        onComplete: () => {
          console.log(`âœ… [TranslationClient] ä»»åŠ¡ ${request.taskId} æµå¼ä¼ è¾“å®Œæˆ`)
        },
        onError: (error: Error) => {
          throw error
        }
      })

      // 5. è®¡ç®—å®é™… Token æ•°
      const inputTokens = await client.countTokens(messages)
      const actualOutputTokens = await this.countTokensAccurate(translation)

      // 6. è®¡ç®—è´¹ç”¨
      const cost = this.calculateCost(inputTokens, actualOutputTokens)

      // 7. æ„å»ºç»“æœ
      const result: TranslationResult = {
        taskId: request.taskId,
        translation,
        inputTokens,
        outputTokens: actualOutputTokens,
        durationMs: Date.now() - startTime,
        cost
      }

      // 8. å®Œæˆå›è°ƒ
      callbacks.onComplete?.(request.taskId, result)
      this.emit('translation:complete', result)

      // 9. æ¸…ç†å®¢æˆ·ç«¯
      await this.cleanup()

      return result

    } catch (error) {
      const err = error as Error
      const errorType = this.classifyError(err)

      // é”™è¯¯å¤„ç†
      callbacks.onError?.(request.taskId, err)
      this.emit('translation:error', {
        taskId: request.taskId,
        errorType,
        errorMessage: err.message
      })

      // æ¸…ç†èµ„æº
      await this.cleanup()

      throw err
    }
  }

  /**
   * æ‰§è¡Œå•æ¬¡ç¿»è¯‘ï¼ˆéæµå¼ï¼‰
   */
  async translate(request: TranslationRequest): Promise<TranslationResult> {
    const startTime = Date.now()

    try {
      const client = await this.initClient()

      const messages = [
        { 
          id: 'system', 
          role: 'system' as const, 
          content: this.config.systemPrompt,
          timestamp: new Date()
        },
        { 
          id: 'user', 
          role: 'user' as const, 
          content: request.content,
          timestamp: new Date()
        }
      ]

      // éæµå¼è°ƒç”¨
      const translation = await client.chat(messages)

      // è®¡ç®— Token
      const inputTokens = await client.countTokens(messages)
      const outputTokens = await this.countTokensAccurate(translation)
      const cost = this.calculateCost(inputTokens, outputTokens)

      const result: TranslationResult = {
        taskId: request.taskId,
        translation,
        inputTokens,
        outputTokens,
        durationMs: Date.now() - startTime,
        cost
      }

      await this.cleanup()

      return result

    } catch (error) {
      await this.cleanup()
      throw error
    }
  }

  /**
   * åˆå§‹åŒ– LangChain å®¢æˆ·ç«¯
   */
  private async initClient(): Promise<LangChainClient> {
    if (this.activeClient) {
      return this.activeClient
    }

    console.log(`ğŸ”§ [TranslationClient] åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ŒmodelId: ${this.config.modelId}`)
    
    // ä» LlmConfigManager è·å–æ¨¡å‹é…ç½®
    const modelConfig = await this.getModelConfig(this.config.modelId)
    console.log(`ğŸ”§ [TranslationClient] è·å–æ¨¡å‹é…ç½®:`, {
      modelName: modelConfig.modelName,
      apiKey: modelConfig.apiKey?.substring(0, 10) + '***',
      baseUrl: modelConfig.baseUrl
    })

    // åŠ¨æ€å¯¼å…¥ LangChainClient
    const { LangChainClient } = await import('../llm-chat-service/langchain-client')
    
    const clientConfig = {
      modelName: modelConfig.modelName,
      apiKey: modelConfig.apiKey,
      baseUrl: modelConfig.baseUrl,
      temperature: this.config.temperature,
      maxTokens: this.config.maxTokens,
      timeout: this.config.timeout,
      maxRetries: this.config.maxRetries,
      useChat: true
    }
    
    console.log(`ğŸ”§ [TranslationClient] LangChainClient é…ç½®:`, {
      modelName: clientConfig.modelName,
      temperature: clientConfig.temperature,
      maxTokens: clientConfig.maxTokens
    })

    this.activeClient = new LangChainClient(clientConfig)
    return this.activeClient
  }
  
  /**
   * è·å–æ¨¡å‹é…ç½®
   */
  private async getModelConfig(modelId: string): Promise<ModelConfig> {
    console.log(`ğŸ” [TranslationClient] è§£æ modelId: ${modelId}`)
    
    // æŒ‰ç¬¬ä¸€ä¸ªç‚¹å·åˆ†å‰²ï¼Œå› ä¸º modelName å¯èƒ½åŒ…å«ç‚¹å·
    // æ ¼å¼ï¼šproviderId.modelName
    const dotIndex = modelId.indexOf('.')
    if (dotIndex === -1) {
      throw new Error(`Invalid modelId format: ${modelId}. Expected format: providerId.modelName`)
    }
    
    const providerId = modelId.substring(0, dotIndex)
    const modelName = modelId.substring(dotIndex + 1)
    
    console.log(`ğŸ” [TranslationClient] åˆ†å‰²ç»“æœ: providerId=${providerId}, modelName=${modelName}`)
    
    // è°ƒç”¨ LlmConfigManager çš„æ–¹æ³•è·å–æä¾›å•†é…ç½®
    const provider = await this.llmConfigManager.getProvider(providerId)
    console.log(`ğŸ” [TranslationClient] è·å–æä¾›å•† ${providerId}:`, provider ? 'æˆåŠŸ' : 'å¤±è´¥')

    if (!provider) {
      throw new Error(`Provider ${providerId} not found`)
    }

    const config = {
      modelName,
      apiKey: provider.apiKey,
      baseUrl: provider.baseUrl,
      ...provider.defaultConfig
    }
    
    console.log(`ğŸ” [TranslationClient] æœ€ç»ˆæ¨¡å‹é…ç½®:`, {
      modelName: config.modelName,
      baseUrl: config.baseUrl
    })
    
    return config
  }

  /**
   * ä¼°ç®— Token æ•°ï¼ˆå¿«é€Ÿä¼°ç®—ï¼Œ1 token â‰ˆ 4 å­—ç¬¦ï¼‰
   */
  private estimateTokens(text: string): number {
    return Math.ceil(text.length / 4)
  }

  /**
   * ç²¾ç¡®è®¡ç®— Token æ•°
   */
  private async countTokensAccurate(text: string): Promise<number> {
    if (!this.activeClient) {
      return this.estimateTokens(text)
    }
    
    const message = { 
      id: 'temp', 
      role: 'user' as const, 
      content: text,
      timestamp: new Date()
    }
    
    return await this.activeClient.countTokens([message])
  }

  /**
   * è®¡ç®—è´¹ç”¨
   * 
   * @description
   * ç¤ºä¾‹å®šä»·ï¼ˆOpenAI GPT-4ï¼‰ï¼Œå®é™…åº”æ ¹æ®å…·ä½“æ¨¡å‹åŠ¨æ€è®¡ç®—
   */
  private calculateCost(inputTokens: number, outputTokens: number): number {
    // TODO: ä»æ¨¡å‹é…ç½®ä¸­è·å–å®šä»·ä¿¡æ¯
    const inputCost = (inputTokens / 1000) * 0.03  // $0.03 per 1K input tokens
    const outputCost = (outputTokens / 1000) * 0.06 // $0.06 per 1K output tokens
    return inputCost + outputCost
  }

  /**
   * é”™è¯¯åˆ†ç±»
   */
  private classifyError(error: Error): ErrorType {
    const message = error.message.toLowerCase()

    if (message.includes('429') || message.includes('rate limit')) {
      return 'RATE_LIMIT'
    }
    if (message.includes('timeout')) {
      return 'TIMEOUT'
    }
    if (message.includes('network') || message.includes('econnrefused')) {
      return 'NETWORK'
    }
    if (message.includes('api key') || message.includes('unauthorized')) {
      return 'INVALID_API_KEY'
    }
    if (message.includes('model')) {
      return 'MODEL_ERROR'
    }

    return 'UNKNOWN'
  }

  /**
   * æ¸…ç†èµ„æº
   */
  private async cleanup(): Promise<void> {
    this.activeClient = null
    this.removeAllListeners()
  }
}

