/**
 * LlmTranslate ä¸“ç”¨çŠ¶æ€ç®¡ç†
 * ä½¿ç”¨Piniaåˆ›å»ºç‹¬ç«‹çš„store
 */

import { defineStore } from 'pinia'
import { ref, computed } from 'vue'
import { mockBatchList, mockTaskList } from './mock'
import type { Batch, Task, TranslateConfig, TaskFilter } from '../types'

// â­ æ— åç«¯æœåŠ¡å™¨ï¼Œæ‰€æœ‰æ“ä½œé€šè¿‡ IPC åœ¨æœ¬åœ°è¿›è¡Œ
// æ¸²æŸ“è¿›ç¨‹ â†â†’ IPC Bridge â†â†’ ä¸»è¿›ç¨‹ (LlmTranslateService)

export const useLlmTranslateStore = defineStore('llmTranslate', () => {
  // â­ å•ä¸€å¼€å…³ï¼šæ”¹è¿™é‡Œåˆ‡æ¢ Mock â†” Electron æœ¬åœ°æœåŠ¡
  const useMock = ref(true) // é»˜è®¤trueä¾¿äºå¼€å‘æµ‹è¯•

  // ==================== çŠ¶æ€å®šä¹‰ ====================
  
  /** é…ç½®çŠ¶æ€ */
  const config = ref<TranslateConfig>({
    inputSource: 'file',
    content: '',
    filePath: '',
    systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆè‹±æ–‡ã€‚',
    chunkStrategy: 'line',
    chunkSize: 50,
    concurrency: 3,
    replyMode: 'predicted',
    predictedTokens: 2000,
    modelId: 'gpt-4',
    outputDir: 'D:\\output\\translate\\'
  })

  /** æ‰¹æ¬¡åˆ—è¡¨ */
  const batchList = ref<Batch[]>([])

  /** å½“å‰æ‰¹æ¬¡ */
  const currentBatch = ref<Batch | null>(null)

  /** ä»»åŠ¡åˆ—è¡¨ */
  const taskList = ref<Task[]>([])

  /** é€‰ä¸­çš„ä»»åŠ¡IDåˆ—è¡¨ */
  const selectedTasks = ref<string[]>([])
  
  /** é€‰ä¸­çš„ä»»åŠ¡IDé›†åˆ */
  const selectedTaskIds = ref<Set<string>>(new Set())

  /** ä»»åŠ¡è¿‡æ»¤å™¨ */
  const taskFilters = ref<TaskFilter>({
    status: ['queued', 'waiting', 'throttled', 'error', 'unsent'],
    searchText: '',
    selectMode: false
  })

  /** çº¿ç¨‹è¯¦æƒ…æŠ½å±‰ */
  const threadDrawer = ref({
    isOpen: false,
    currentTaskId: null as string | null
  })

  /** åŠ è½½çŠ¶æ€ */
  const loading = ref(false)

  /** é”™è¯¯ä¿¡æ¯ */
  const error = ref<string | null>(null)

  // ==================== è®¡ç®—å±æ€§ ====================

  /** æ‰¹æ¬¡ç»Ÿè®¡ */
  const batchStats = computed(() => {
    if (!currentBatch.value) return null
    
    const batch = currentBatch.value
    return {
      totalTasks: batch.totalTasks,
      completedTasks: batch.completedTasks,
      failedTasks: batch.failedTasks,
      successRate: batch.totalTasks > 0 
        ? Math.round((batch.completedTasks / batch.totalTasks) * 100)
        : 0
    }
  })

  /** è¿‡æ»¤åçš„ä»»åŠ¡åˆ—è¡¨ */
  const filteredTaskList = computed(() => {
    return taskList.value.filter(task => {
      const statusMatch = taskFilters.value.status.includes(task.status)
      const searchMatch = !taskFilters.value.searchText ||
        task.id.includes(taskFilters.value.searchText) ||
        task.content.includes(taskFilters.value.searchText)
      return statusMatch && searchMatch
    })
  })

  /** Token ä¼°ç®— */
  const tokenEstimate = computed(() => {
    const inputTokens = estimateTokens(config.value.content)
    const systemPromptTokens = estimateTokens(config.value.systemPrompt)
    const totalTokens = inputTokens + systemPromptTokens
    const estimatedCost = calculateCost(totalTokens, config.value.modelId)

    return {
      inputTokens,
      systemPromptTokens,
      totalTokens,
      estimatedCost
    }
  })

  // ==================== æ•°æ®è·å–æ–¹æ³• ====================

  /**
   * è·å–æ‰¹æ¬¡åˆ—è¡¨
   * æ”¯æŒ Mock/IPC åˆ‡æ¢
   */
  const fetchBatchList = async () => {
    loading.value = true
    error.value = null

    try {
      if (useMock.value) {
        batchList.value = mockBatchList
      } else {
        // é€šè¿‡ IPC è°ƒç”¨ä¸»è¿›ç¨‹çš„ getBatches()
        const result = await window.nimbria.llmTranslate.getBatches()
        if (result.success && result.data) {
          batchList.value = result.data.batches
        } else {
          throw new Error(result.error || 'è·å–æ‰¹æ¬¡åˆ—è¡¨å¤±è´¥')
        }
      }
    } catch (err) {
      error.value = err instanceof Error ? err.message : 'è·å–æ‰¹æ¬¡åˆ—è¡¨å¤±è´¥'
      console.error('Failed to fetch batch list:', err)
    } finally {
      loading.value = false
    }
  }

  /**
   * è·å–ä»»åŠ¡åˆ—è¡¨
   * æ”¯æŒ Mock/IPC åˆ‡æ¢
   */
  const fetchTaskList = async (batchId: string) => {
    loading.value = true
    error.value = null

    try {
      if (useMock.value) {
        taskList.value = mockTaskList.filter(t => t.batchId === batchId)
      } else {
        // é€šè¿‡ IPC è°ƒç”¨ä¸»è¿›ç¨‹çš„ getTasks()
        const result = await window.nimbria.llmTranslate.getTasks({ batchId })
        if (result.success && result.data) {
          taskList.value = result.data.tasks
        } else {
          throw new Error(result.error || 'è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥')
        }
      }

      // è®¡ç®—è¿›åº¦
      taskList.value.forEach(task => {
        task.progress = calculateProgress(task.replyTokens, task.predictedTokens)
      })
    } catch (err) {
      error.value = err instanceof Error ? err.message : 'è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥'
      console.error('Failed to fetch task list:', err)
    } finally {
      loading.value = false
    }
  }

  /**
   * åˆ›å»ºæ–°æ‰¹æ¬¡
   */
  const createBatch = async (configData: TranslateConfig) => {
    loading.value = true
    error.value = null

    try {
      if (useMock.value) {
        // Mock æ¨¡å¼ï¼šæ¨¡æ‹Ÿåˆ›å»ºæ‰¹æ¬¡
        const newBatch: Batch = {
          id: `#${Date.now().toString().slice(-8)}`,
          status: 'running',
          configJson: JSON.stringify(configData),
          totalTasks: 0,
          completedTasks: 0,
          failedTasks: 0,
          throttledTasks: 0,
          waitingTasks: 0,
          unsentTasks: 0,
          terminatedTasks: 0,
          totalCost: 0,
          totalInputTokens: 0,
          totalOutputTokens: 0,
          avgTimePerTask: 0,
          fastestTaskTime: 0,
          slowestTaskTime: 0,
          estimatedCompletionTime: null,
          createdAt: new Date().toISOString(),
          startedAt: null,
          completedAt: null,
          updatedAt: new Date().toISOString()
        }
        currentBatch.value = newBatch
        batchList.value.unshift(newBatch)
        return newBatch
      } else {
        // é€šè¿‡ IPC è°ƒç”¨ä¸»è¿›ç¨‹åˆ›å»ºæ‰¹æ¬¡
        const result = await window.nimbria.llmTranslate.createBatch({
          config: configData,
          content: configData.content
        })
        
        if (!result.success || !result.data?.batchId) {
          throw new Error(result.error || 'åˆ›å»ºæ‰¹æ¬¡å¤±è´¥')
        }

        // ç­‰å¾…æ‰¹æ¬¡åˆ›å»ºå®Œæˆäº‹ä»¶ï¼ˆé€šè¿‡ Promise + äº‹ä»¶ç›‘å¬ï¼‰
        return await new Promise<Batch>((resolve, reject) => {
          const timeout = setTimeout(() => reject(new Error('åˆ›å»ºè¶…æ—¶')), 30000)
          
          const handler = (data: any) => {
            if (data.batchId === result.data?.batchId) {
              clearTimeout(timeout)
              resolve(data.batch)
            }
          }
          
          window.nimbria.llmTranslate.onBatchCreated(handler)
        })
      }
    } catch (err) {
      error.value = err instanceof Error ? err.message : 'åˆ›å»ºæ‰¹æ¬¡å¤±è´¥'
      console.error('Failed to create batch:', err)
      throw err
    } finally {
      loading.value = false
    }
  }

  /**
   * é‡è¯•å¤±è´¥çš„ä»»åŠ¡
   */
  const retryFailedTasks = async (batchId: string) => {
    try {
      if (useMock.value) {
        // Mock æ¨¡å¼ï¼šæ¨¡æ‹Ÿé‡è¯•
        const failedTasks = taskList.value.filter(t => t.status === 'error' || t.status === 'throttled')
        failedTasks.forEach(task => {
          task.status = 'waiting'
          task.retryCount++
        })
      } else {
        // é€šè¿‡ IPC è°ƒç”¨ä¸»è¿›ç¨‹é‡è¯•
        const result = await window.nimbria.llmTranslate.retryFailedTasks({ batchId })
        if (result.success) {
          await fetchTaskList(batchId)
        } else {
          throw new Error(result.error || 'é‡è¯•å¤±è´¥')
        }
      }
    } catch (err) {
      console.error('Failed to retry tasks:', err)
    }
  }

  // ==================== å·¥å…·æ–¹æ³• ====================

  /**
   * è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
   */
  const calculateProgress = (replyTokens: number, predictedTokens: number): number => {
    if (predictedTokens === 0) return 0
    return Math.min(Math.round((replyTokens / predictedTokens) * 100), 100)
  }

  /**
   * ä¼°ç®—Tokenæ•°é‡
   * ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦0.5ä¸ªå­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4ä¸ªå­—ç¬¦/token
   */
  const estimateTokens = (text: string): number => {
    if (!text) return 0
    const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length
    const englishWords = text.split(/\s+/).filter(w => /[a-zA-Z]/.test(w)).length
    return Math.ceil(chineseChars * 0.5 + englishWords * 0.25)
  }

  /**
   * è®¡ç®—è´¹ç”¨
   */
  const calculateCost = (tokens: number, modelId: string): number => {
    // ç®€å•å®šä»·è¡¨ï¼ˆå®é™…åº”è¯¥ä»é…ç½®è¯»å–ï¼‰
    const pricePerMillion = modelId === 'gpt-4' ? 30 : 2
    return (tokens / 1000000) * pricePerMillion
  }

  /**
   * è®¡ç®—é¢„è®¡å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰
   */
  const calculateEstimatedTime = (
    completedTasks: number,
    totalTasks: number,
    avgTimePerTask: number
  ): number => {
    const remainingTasks = totalTasks - completedTasks
    return remainingTasks * avgTimePerTask
  }

  // ==================== Actions ====================

  /** åˆå§‹åŒ–æ•°æ® */
  const initialize = async () => {
    await fetchBatchList()
    if (batchList.value.length > 0) {
      const firstBatch = batchList.value[0]
      if (firstBatch) {
        currentBatch.value = firstBatch
        await fetchTaskList(firstBatch.id)
      }
    }
  }

  /** åˆ‡æ¢å½“å‰æ‰¹æ¬¡ */
  const switchBatch = async (batchId: string) => {
    const batch = batchList.value.find(b => b.id === batchId)
    if (batch) {
      currentBatch.value = batch
      await fetchTaskList(batchId)
    }
  }

  /** æ‰“å¼€çº¿ç¨‹è¯¦æƒ… */
  const openThreadDrawer = (taskId: string) => {
    threadDrawer.value = {
      isOpen: true,
      currentTaskId: taskId
    }
  }

  /** å…³é—­çº¿ç¨‹è¯¦æƒ… */
  const closeThreadDrawer = () => {
    threadDrawer.value = {
      isOpen: false,
      currentTaskId: null
    }
  }

  // ==================== äº‹ä»¶ç›‘å¬å™¨è®¾ç½® ====================

  /**
   * è®¾ç½® IPC äº‹ä»¶ç›‘å¬å™¨
   * åœ¨éMockæ¨¡å¼ä¸‹è°ƒç”¨ï¼Œç›‘å¬ä¸»è¿›ç¨‹å‘æ¥çš„äº‹ä»¶
   */
  const setupListeners = () => {
    if (useMock.value) return  // Mock æ¨¡å¼ä¸éœ€è¦ç›‘å¬

    // æ‰¹æ¬¡åˆ›å»ºäº‹ä»¶
    window.nimbria.llmTranslate.onBatchCreateStart((data: any) => {
      console.log('ğŸ“¦ [Store] æ‰¹æ¬¡åˆ›å»ºå¼€å§‹:', data.batchId)
    })

    window.nimbria.llmTranslate.onBatchCreated((data: any) => {
      console.log('âœ… [Store] æ‰¹æ¬¡åˆ›å»ºå®Œæˆ:', data.batchId)
      
      // æ›´æ–°æ‰¹æ¬¡åˆ—è¡¨
      batchList.value.unshift(data.batch)
      currentBatch.value = data.batch
      taskList.value = data.tasks
    })

    window.nimbria.llmTranslate.onBatchCreateError((data: any) => {
      console.error('âŒ [Store] æ‰¹æ¬¡åˆ›å»ºå¤±è´¥:', data.error)
      error.value = data.error
    })

    // ä»»åŠ¡è¿›åº¦äº‹ä»¶
    window.nimbria.llmTranslate.onTaskProgress((data: any) => {
      const task = taskList.value.find(t => t.id === data.taskId)
      if (task) {
        task.replyTokens = data.replyTokens
        task.progress = data.progress
        task.translation = (task.translation || '') + data.chunk
      }
    })

    // ä»»åŠ¡å®Œæˆäº‹ä»¶
    window.nimbria.llmTranslate.onTaskComplete((data: any) => {
      const task = taskList.value.find(t => t.id === data.taskId)
      if (task) {
        task.status = 'completed'
        task.translation = data.translation
        task.progress = 100
        task.replyTokens = data.totalTokens
      }
      
      // æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡
      if (currentBatch.value && currentBatch.value.id === data.batchId) {
        currentBatch.value.completedTasks++
      }
    })

    // ä»»åŠ¡é”™è¯¯äº‹ä»¶
    window.nimbria.llmTranslate.onTaskError((data: any) => {
      const task = taskList.value.find(t => t.id === data.taskId)
      if (task) {
        if (data.errorType === 'rate_limit') {
          task.status = 'throttled'
        } else {
          task.status = 'error'
        }
        task.errorMessage = data.errorMessage
        task.errorType = data.errorType
      }
      
      // æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡
      if (currentBatch.value && currentBatch.value.id === data.batchId) {
        if (data.errorType === 'rate_limit') {
          currentBatch.value.throttledTasks++
        } else {
          currentBatch.value.failedTasks++
        }
      }
    })

    // å¯¼å‡ºäº‹ä»¶
    window.nimbria.llmTranslate.onExportComplete((data: any) => {
      console.log('âœ… [Store] å¯¼å‡ºå®Œæˆ:', data.filePath)
    })

    window.nimbria.llmTranslate.onExportError((data: any) => {
      console.error('âŒ [Store] å¯¼å‡ºå¤±è´¥:', data.error)
      error.value = data.error
    })
  }

  return {
    // çŠ¶æ€
    config,
    batchList,
    currentBatch,
    taskList,
    selectedTasks,
    selectedTaskIds,
    taskFilters,
    threadDrawer,
    loading,
    error,
    useMock,

    // è®¡ç®—å±æ€§
    batchStats,
    filteredTaskList,
    tokenEstimate,

    // æ–¹æ³•
    fetchBatchList,
    fetchTaskList,
    createBatch,
    retryFailedTasks,
    calculateProgress,
    estimateTokens,
    calculateCost,
    calculateEstimatedTime,
    initialize,
    switchBatch,
    openThreadDrawer,
    closeThreadDrawer,
    setupListeners
  }
})

