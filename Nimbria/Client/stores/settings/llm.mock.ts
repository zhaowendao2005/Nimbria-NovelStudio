/**
 * LLMæä¾›å•†Mockæ•°æ®
 * æ¨¡æ‹Ÿ3ä¸ªæä¾›å•†ï¼šOpenAIã€Anthropicã€Custom Provider
 */

import type { ModelProvider, ActiveModelConfig } from './types';
import { DEFAULT_MODEL_CONFIG } from './types';

/**
 * Mockæ•°æ®ï¼šOpenAI Provider (activeçŠ¶æ€)
 */
export const openAIProviderMock: ModelProvider = {
  id: 'openai',
  name: 'openai',
  displayName: 'OpenAI',
  description: 'Official OpenAI API provider with GPT-3.5 and GPT-4 models',
  logo: 'ğŸ¤–',
  status: 'active',
  apiKey: 'sk-mock-openai-key-xxxxxxxxxxxxx',
  baseUrl: 'https://api.openai.com/v1',
  defaultConfig: {
    ...DEFAULT_MODEL_CONFIG,
    contextLength: 8192,
    maxTokens: 4096,
    functionCalling: 'æ”¯æŒ',
    structuredOutput: 'æ”¯æŒ',
  },
  supportedModels: [
    {
      type: 'LLM',
      models: [
        {
          name: 'gpt-3.5-turbo',
          config: {
            contextLength: 4096,
            maxTokens: 4096,
            functionCalling: 'æ”¯æŒ',
          }
        },
        {
          name: 'gpt-3.5-turbo-16k',
          config: {
            contextLength: 16384,
            maxTokens: 16384,
            functionCalling: 'æ”¯æŒ',
          }
        },
        {
          name: 'gpt-4',
          config: {
            contextLength: 8192,
            maxTokens: 8192,
            functionCalling: 'æ”¯æŒ',
            structuredOutput: 'æ”¯æŒ',
          }
        },
        {
          name: 'gpt-4-turbo',
          config: {
            contextLength: 128000,
            maxTokens: 4096,
            functionCalling: 'æ”¯æŒ',
            structuredOutput: 'æ”¯æŒ',
          }
        },
        {
          name: 'gpt-4o',
          config: {
            contextLength: 128000,
            maxTokens: 16384,
            functionCalling: 'æ”¯æŒ',
            structuredOutput: 'æ”¯æŒ',
          }
        }
      ]
    },
    {
      type: 'TEXT_EMBEDDING',
      models: [
        {
          name: 'text-embedding-ada-002',
          config: {
            contextLength: 8191,
            maxTokens: 8191,
          }
        },
        {
          name: 'text-embedding-3-small',
          config: {
            contextLength: 8191,
            maxTokens: 8191,
          }
        },
        {
          name: 'text-embedding-3-large',
          config: {
            contextLength: 8191,
            maxTokens: 8191,
          }
        }
      ]
    }
  ],
  lastRefreshed: new Date(),
  refreshStatus: 'success',
};

/**
 * Mockæ•°æ®ï¼šAnthropic Provider (inactiveçŠ¶æ€)
 */
export const anthropicProviderMock: ModelProvider = {
  id: 'anthropic',
  name: 'anthropic',
  displayName: 'Anthropic',
  description: 'Claude AI models by Anthropic with long context support',
  logo: 'ğŸ§ ',
  status: 'inactive',
  apiKey: 'sk-ant-mock-key-xxxxxxxxxxxxx',
  baseUrl: 'https://api.anthropic.com/v1',
  defaultConfig: {
    ...DEFAULT_MODEL_CONFIG,
    contextLength: 200000,
    maxTokens: 4096,
    functionCalling: 'æ”¯æŒ',
  },
  supportedModels: [
    {
      type: 'LLM',
      models: [
        {
          name: 'claude-3-opus-20240229',
          config: {
            contextLength: 200000,
            maxTokens: 4096,
            functionCalling: 'æ”¯æŒ',
          }
        },
        {
          name: 'claude-3-sonnet-20240229',
          config: {
            contextLength: 200000,
            maxTokens: 4096,
            functionCalling: 'æ”¯æŒ',
          }
        },
        {
          name: 'claude-3-haiku-20240307',
          config: {
            contextLength: 200000,
            maxTokens: 4096,
            functionCalling: 'æ”¯æŒ',
          }
        },
        {
          name: 'claude-3-5-sonnet-20240620',
          config: {
            contextLength: 200000,
            maxTokens: 8192,
            functionCalling: 'æ”¯æŒ',
          }
        }
      ]
    }
  ],
  lastRefreshed: new Date(Date.now() - 1000 * 60 * 30), // 30åˆ†é’Ÿå‰
  refreshStatus: 'idle',
};

/**
 * Mockæ•°æ®ï¼šCustom Provider (availableçŠ¶æ€)
 */
export const customProviderMock: ModelProvider = {
  id: 'custom-local',
  name: 'custom-local',
  displayName: 'Local LLM Server',
  description: 'Self-hosted local LLM server with custom models',
  logo: 'ğŸ ',
  status: 'available',
  apiKey: 'local-api-key',
  baseUrl: 'http://localhost:8000/v1',
  defaultConfig: {
    ...DEFAULT_MODEL_CONFIG,
    timeout: 60000,
    contextLength: 32768,
    maxTokens: 8192,
  },
  supportedModels: [
    {
      type: 'LLM',
      models: [
        {
          name: 'llama-3-70b-instruct',
          config: {
            contextLength: 32768,
            maxTokens: 8192,
            agentThought: 'æ”¯æŒ',
          }
        },
        {
          name: 'qwen-2.5-72b-instruct',
          config: {
            contextLength: 32768,
            maxTokens: 8192,
            functionCalling: 'æ”¯æŒ',
          }
        },
        {
          name: 'mistral-large-2',
          config: {
            contextLength: 131072,
            maxTokens: 8192,
            functionCalling: 'æ”¯æŒ',
          }
        }
      ]
    },
    {
      type: 'TEXT_EMBEDDING',
      models: [
        {
          name: 'bge-large-zh-v1.5',
          config: {
            contextLength: 512,
            maxTokens: 512,
          }
        }
      ]
    }
  ],
  lastRefreshed: undefined,
  refreshStatus: 'idle',
};

/**
 * æ‰€æœ‰Mock Providers
 */
export const llmProvidersMock: ModelProvider[] = [
  openAIProviderMock,
  anthropicProviderMock,
  customProviderMock,
];

/**
 * Mockæ•°æ®ï¼šæ´»åŠ¨æ¨¡å‹é…ç½®
 */
export const activeModelsMock: ActiveModelConfig = {
  'LLM': 'openai.gpt-4o',
  'TEXT_EMBEDDING': 'openai.text-embedding-3-large',
};

