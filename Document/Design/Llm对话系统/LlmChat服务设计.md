# Nimbria LLM Chat æœåŠ¡è®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2025å¹´10æœˆ15æ—¥  
**æŠ€æœ¯æ ˆ**: LangChain + Electron + LocalStorage  

---

## ğŸ“‹ ç³»ç»Ÿæ¦‚è¿°

Nimbria çš„ LLM Chat æœåŠ¡æä¾›äº†ä¸€ä¸ªåŸºäº LangChain çš„é€šç”¨å¯¹è¯ç³»ç»Ÿï¼Œæ”¯æŒå¤šæ¨¡å‹å¯¹è¯ã€æµå¼å“åº”ã€å¯¹è¯å†å²ç®¡ç†ã€ä¸Šä¸‹æ–‡æ§åˆ¶ç­‰åŠŸèƒ½ã€‚è¯¥æœåŠ¡åœ¨ Electron ä¸»è¿›ç¨‹ä¸­è¿è¡Œï¼Œé€šè¿‡ IPC ä¸å‰ç«¯é€šä¿¡ï¼Œå¯¹è¯æ•°æ®å­˜å‚¨åœ¨ LocalStorage ä¸­ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **LangChain é›†æˆ**: ä½¿ç”¨ LangChain çš„ ChatOpenAI å’Œç›¸å…³ç»„ä»¶
- **å¤šæ¨¡å‹æ”¯æŒ**: åŸºäºç°æœ‰çš„æ´»è·ƒæ¨¡å‹é…ç½®ç³»ç»Ÿ
- **æµå¼å“åº”**: æ”¯æŒå®æ—¶æµå¼è¾“å‡º
- **å¯¹è¯ç®¡ç†**: åˆ›å»ºã€åˆ é™¤ã€åˆ‡æ¢å¯¹è¯
- **å†å²ç®¡ç†**: è‡ªåŠ¨ä¿å­˜å¯¹è¯å†å²åˆ° LocalStorage
- **ä¸Šä¸‹æ–‡æ§åˆ¶**: Token è®¡æ•°å’Œè‡ªåŠ¨è£å‰ª
- **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **æ¨¡å‹é…ç½®é›†æˆ**: åŸºäºç°æœ‰ModelConfigçš„æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TD
    A[å‰ç«¯ LlmChatPanel] -->|IPC| B[ä¸»è¿›ç¨‹ LlmChatHandlers]
    B --> C[LlmChatService]
    C --> D[LangChainClient]
    D --> E[LangChain ChatOpenAI]
    E --> F[OpenAI API]
    
    C --> G[ConversationManager]
    G --> H[LocalStorage via IPC]
    
    C --> I[LlmConfigManager]
    I --> J[è·å–æ´»è·ƒæ¨¡å‹é…ç½®]
    
    subgraph Frontend[å‰ç«¯æ¸²æŸ“è¿›ç¨‹]
        A
        H
    end
    
    subgraph MainProcess[ä¸»è¿›ç¨‹]
        B
        C
        D
        G
        I
    end
    
    subgraph External[å¤–éƒ¨æœåŠ¡]
        E
        F
    end
```

### æ•°æ®æµ

```
ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
    â†“
å‰ç«¯ ChatInput ç»„ä»¶
    â†“
llmChatStore.sendMessage()
    â†“
IPC: llm-chat:send-message
    â†“
ä¸»è¿›ç¨‹ LlmChatHandlers
    â†“
LlmChatService.sendMessage()
    â†“
LangChainClient.chat() [æµå¼]
    â†“
é€å—è¿”å›å†…å®¹
    â†“
IPC: llm-chat:stream-chunk
    â†“
å‰ç«¯æ›´æ–° UI
    â†“
å¯¹è¯å®Œæˆï¼Œä¿å­˜åˆ° LocalStorage
```

---

## ğŸ“ æ–‡ä»¶æ¶æ„

```
Nimbria/
â”œâ”€â”€ src-electron/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ llm-chat-service/
â”‚   â”‚       â”œâ”€â”€ llm-chat-service.ts          # ä¸»æœåŠ¡ç±»
â”‚   â”‚       â”œâ”€â”€ langchain-client.ts          # LangChain å®¢æˆ·ç«¯å°è£…
â”‚   â”‚       â”œâ”€â”€ conversation-manager.ts      # å¯¹è¯ç®¡ç†å™¨
â”‚   â”‚       â”œâ”€â”€ context-manager.ts           # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
â”‚   â”‚       â”œâ”€â”€ types.ts                     # ç±»å‹å®šä¹‰
â”‚   â”‚       â””â”€â”€ utils.ts                     # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ ipc/main-renderer/
â”‚   â”‚   â””â”€â”€ llm-chat-handlers.ts             # IPC å¤„ç†å™¨
â”‚   â””â”€â”€ core/
â”‚       â””â”€â”€ app-manager.ts                   # æ³¨å†ŒæœåŠ¡
â”œâ”€â”€ Client/
â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â””â”€â”€ llmChat/
â”‚   â”‚       â”œâ”€â”€ llmChatStore.ts              # å‰ç«¯ LlmChat Store
â”‚   â”‚       â””â”€â”€ types.ts                     # å‰ç«¯ç±»å‹å®šä¹‰
â”‚   â””â”€â”€ GUI/components/ProjectPage.Shell/Navbar.content/LlmChat/
â”‚       â”œâ”€â”€ LlmChatPanel.vue                 # ä¸»é¢æ¿
â”‚       â”œâ”€â”€ ChatTabs.vue                     # å¯¹è¯æ ‡ç­¾é¡µ
â”‚       â”œâ”€â”€ ChatMessages.vue                 # æ¶ˆæ¯åˆ—è¡¨
â”‚       â”œâ”€â”€ ChatInput.vue                    # è¾“å…¥æ¡†
â”‚       â””â”€â”€ ModelSelector.vue                # æ¨¡å‹é€‰æ‹©å™¨
â””â”€â”€ package.json                             # æ·»åŠ  LangChain ä¾èµ–
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 1. LangChainClient (LangChain å®¢æˆ·ç«¯)

**èŒè´£**: å°è£… LangChain çš„è°ƒç”¨é€»è¾‘

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class LangChainClient {
  // åˆå§‹åŒ–å®¢æˆ·ç«¯
  constructor(config: {
    modelName: string
    apiKey: string
    baseUrl: string
    temperature?: number
    maxTokens?: number
  })
  
  // å‘é€æ¶ˆæ¯ï¼ˆæµå¼ï¼‰
  async chatStream(
    messages: ChatMessage[],
    onChunk: (chunk: string) => void,
    onComplete: () => void,
    onError: (error: Error) => void
  ): Promise<void>
  
  // å‘é€æ¶ˆæ¯ï¼ˆéæµå¼ï¼‰
  async chat(messages: ChatMessage[]): Promise<string>
  
  // è®¡ç®— Token æ•°é‡
  async countTokens(messages: ChatMessage[]): Promise<number>
}
```

**LangChain ç»„ä»¶ä½¿ç”¨**:
- `ChatOpenAI`: åŸºç¡€èŠå¤©æ¨¡å‹
- `HumanMessage`, `AIMessage`, `SystemMessage`: æ¶ˆæ¯ç±»å‹
- `CallbackManager`: æµå¼å›è°ƒç®¡ç†
- `encoding_for_model`: Token è®¡æ•°

**ModelConfig é›†æˆ**:
- ä» `LlmConfigManager` è·å–æ¨¡å‹çš„ `contextLength`ã€`maxTokens` ç­‰é…ç½®
- æ ¹æ® `completionMode` é€‰æ‹©ä½¿ç”¨ Chat æˆ– Completion API
- ä½¿ç”¨ `timeout` å’Œ `maxRetries` è¿›è¡Œé”™è¯¯å¤„ç†
- åº”ç”¨ `systemPromptSeparator` æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯

---

### 2. ConversationManager (å¯¹è¯ç®¡ç†å™¨)

**èŒè´£**: ç®¡ç†å¯¹è¯çš„åˆ›å»ºã€åˆ é™¤ã€å†å²è®°å½•

**æ•°æ®ç»“æ„**:
```typescript
interface Conversation {
  id: string                    // å¯¹è¯ID
  title: string                 // å¯¹è¯æ ‡é¢˜
  modelId: string               // ä½¿ç”¨çš„æ¨¡å‹ID (providerId.modelName)
  messages: ChatMessage[]       // æ¶ˆæ¯åˆ—è¡¨
  createdAt: Date              // åˆ›å»ºæ—¶é—´
  updatedAt: Date              // æ›´æ–°æ—¶é—´
  settings: ConversationSettings // å¯¹è¯è®¾ç½®
}

interface ChatMessage {
  id: string
  role: 'system' | 'user' | 'assistant'
  content: string
  timestamp: Date
  metadata?: {
    fileReferences?: FileReference[]  // æ–‡ä»¶å¼•ç”¨
    tokenCount?: number               // Token æ•°é‡
  }
}

interface ConversationSettings {
  temperature: number           // æ¸©åº¦å‚æ•° (0-1)
  maxTokens: number            // æœ€å¤§ç”Ÿæˆ Token (ä»ModelConfigç»§æ‰¿)
  systemPrompt?: string        // ç³»ç»Ÿæç¤ºè¯
  contextWindow: number        // ä¸Šä¸‹æ–‡çª—å£å¤§å° (ä»ModelConfig.contextLengthç»§æ‰¿)
  
  // ä» ModelConfig ç»§æ‰¿çš„è®¾ç½®
  timeout: number              // è¯·æ±‚è¶…æ—¶ (ms)
  maxRetries: number           // æœ€å¤§é‡è¯•æ¬¡æ•°
  completionMode: 'å¯¹è¯' | 'è¡¥å…¨' // APIæ¨¡å¼
}
```

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class ConversationManager {
  // åˆ›å»ºæ–°å¯¹è¯
  async createConversation(
    modelId: string,
    settings?: Partial<ConversationSettings>
  ): Promise<Conversation>
  
  // è·å–å¯¹è¯
  async getConversation(conversationId: string): Promise<Conversation | null>
  
  // è·å–æ‰€æœ‰å¯¹è¯åˆ—è¡¨
  async getAllConversations(): Promise<Conversation[]>
  
  // åˆ é™¤å¯¹è¯
  async deleteConversation(conversationId: string): Promise<void>
  
  // æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯
  async addMessage(
    conversationId: string,
    message: Omit<ChatMessage, 'id' | 'timestamp'>
  ): Promise<ChatMessage>
  
  // æ›´æ–°å¯¹è¯æ ‡é¢˜
  async updateTitle(conversationId: string, title: string): Promise<void>
  
  // æ›´æ–°å¯¹è¯è®¾ç½®
  async updateSettings(
    conversationId: string,
    settings: Partial<ConversationSettings>
  ): Promise<void>
  
  // æ¸…ç©ºå¯¹è¯æ¶ˆæ¯
  async clearMessages(conversationId: string): Promise<void>
  
  // ä¿å­˜åˆ° LocalStorage
  private async saveToStorage(): Promise<void>
  
  // ä» LocalStorage åŠ è½½
  private async loadFromStorage(): Promise<void>
}
```

**LocalStorage å­˜å‚¨ç»“æ„**:
```typescript
// Key: nimbria_llm_conversations
{
  conversations: Conversation[]
}
```

---

### 3. ContextManager (ä¸Šä¸‹æ–‡ç®¡ç†å™¨)

**èŒè´£**: ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢è¶…å‡º Token é™åˆ¶

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class ContextManager {
  // è£å‰ªæ¶ˆæ¯åˆ—è¡¨ä»¥é€‚åº”ä¸Šä¸‹æ–‡çª—å£
  async trimMessages(
    messages: ChatMessage[],
    maxTokens: number,
    preserveSystemPrompt: boolean = true
  ): Promise<ChatMessage[]>
  
  // è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€» Token æ•°
  async calculateTotalTokens(messages: ChatMessage[]): Promise<number>
  
  // æ™ºèƒ½è£å‰ªç­–ç•¥ (åŸºäºModelConfig.contextLength)
  // 1. ä¿ç•™ç³»ç»Ÿæç¤ºè¯
  // 2. ä¿ç•™æœ€æ–°çš„æ¶ˆæ¯
  // 3. ä»æœ€æ—§çš„æ¶ˆæ¯å¼€å§‹åˆ é™¤ï¼Œä¿æŒå¯¹è¯è¿è´¯æ€§
  async smartTrim(
    messages: ChatMessage[],
    maxTokens: number
  ): Promise<ChatMessage[]>
}
```

**è£å‰ªç­–ç•¥**:
```
1. å§‹ç»ˆä¿ç•™ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰
2. å§‹ç»ˆä¿ç•™æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
3. ä»æœ€æ—§çš„æ¶ˆæ¯å¼€å§‹åˆ é™¤ï¼Œç›´åˆ°æ»¡è¶³ Token é™åˆ¶
4. ä¿ç•™å¯¹è¯çš„è¿è´¯æ€§ï¼ˆæˆå¯¹ä¿ç•™ user-assistantï¼‰
```

---

### 4. LlmChatService (ä¸»æœåŠ¡ç±»)

**èŒè´£**: åè°ƒå„ä¸ªç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€çš„æœåŠ¡æ¥å£

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class LlmChatService {
  private conversationManager: ConversationManager
  private contextManager: ContextManager
  private llmConfigManager: LlmConfigManager
  private activeClients: Map<string, LangChainClient>
  
  // åˆå§‹åŒ–æœåŠ¡
  async initialize(): Promise<void>
  
  // åˆ›å»ºæ–°å¯¹è¯
  async createConversation(
    modelId: string,
    settings?: Partial<ConversationSettings>
  ): Promise<Conversation>
  
  // å‘é€æ¶ˆæ¯ï¼ˆæµå¼ï¼‰
  async sendMessage(
    conversationId: string,
    content: string,
    options?: {
      onChunk?: (chunk: string) => void
      onComplete?: () => void
      onError?: (error: Error) => void
    }
  ): Promise<void>
  
  // é‡æ–°ç”Ÿæˆæœ€åä¸€æ¡æ¶ˆæ¯
  async regenerateLastMessage(
    conversationId: string,
    options?: {
      onChunk?: (chunk: string) => void
      onComplete?: () => void
      onError?: (error: Error) => void
    }
  ): Promise<void>
  
  // åˆ é™¤æ¶ˆæ¯
  async deleteMessage(
    conversationId: string,
    messageId: string
  ): Promise<void>
  
  // è·å–å¯¹è¯åˆ—è¡¨
  async getConversations(): Promise<Conversation[]>
  
  // åˆ é™¤å¯¹è¯
  async deleteConversation(conversationId: string): Promise<void>
  
  // æ›´æ–°å¯¹è¯è®¾ç½®
  async updateConversationSettings(
    conversationId: string,
    settings: Partial<ConversationSettings>
  ): Promise<void>
  
  // è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
  async getAvailableModels(): Promise<ActiveModel[]>
  
  // åˆ‡æ¢å¯¹è¯ä½¿ç”¨çš„æ¨¡å‹
  async switchModel(
    conversationId: string,
    modelId: string
  ): Promise<void>
  
  // è·å–æˆ–åˆ›å»º LangChain å®¢æˆ·ç«¯
  private async getOrCreateClient(modelId: string): Promise<LangChainClient>
  
  // å‡†å¤‡å‘é€çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ç®¡ç†ï¼‰
  private async prepareMessages(
    conversation: Conversation,
    newMessage: string
  ): Promise<ChatMessage[]>
}
```

---

## ğŸ”§ ModelConfig é›†æˆè®¾è®¡

### ä»ç°æœ‰ç³»ç»Ÿè·å–æ¨¡å‹é…ç½®

**LlmChatService åˆå§‹åŒ–æ—¶**:
```typescript
class LlmChatService {
  private async getModelConfig(modelId: string): Promise<ModelConfig> {
    // è§£æ modelId: "providerId.modelName"
    const [providerId, modelName] = modelId.split('.')
    
    // ä» LlmConfigManager è·å–æä¾›å•†é…ç½®
    const provider = await this.llmConfigManager.getProvider(providerId)
    if (!provider) {
      throw new Error(`æä¾›å•† ${providerId} ä¸å­˜åœ¨`)
    }
    
    // è·å–æ¨¡å‹ç‰¹å®šé…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
    const modelGroup = provider.supportedModels.find(g => 
      g.models.some(m => m.name === modelName)
    )
    const model = modelGroup?.models.find(m => m.name === modelName)
    
    // åˆå¹¶é…ç½®ï¼šé»˜è®¤é…ç½® + æ¨¡å‹ç‰¹å®šé…ç½®
    return {
      ...provider.defaultConfig,
      ...model?.config
    }
  }
}
```

### å¯¹è¯è®¾ç½®ç»§æ‰¿

**åˆ›å»ºå¯¹è¯æ—¶è‡ªåŠ¨ç»§æ‰¿ModelConfig**:
```typescript
async createConversation(modelId: string, userSettings?: Partial<ConversationSettings>) {
  const modelConfig = await this.getModelConfig(modelId)
  
  // ä» ModelConfig ç»§æ‰¿é»˜è®¤è®¾ç½®
  const defaultSettings: ConversationSettings = {
    temperature: 0.7,                           // ç”¨æˆ·å¯è°ƒæ•´
    maxTokens: modelConfig.maxTokens,           // ä»æ¨¡å‹é…ç½®ç»§æ‰¿
    contextWindow: modelConfig.contextLength,   // ä»æ¨¡å‹é…ç½®ç»§æ‰¿
    timeout: modelConfig.timeout,               // ä»æ¨¡å‹é…ç½®ç»§æ‰¿
    maxRetries: modelConfig.maxRetries,         // ä»æ¨¡å‹é…ç½®ç»§æ‰¿
    completionMode: modelConfig.completionMode, // ä»æ¨¡å‹é…ç½®ç»§æ‰¿
    systemPrompt: undefined                     // ç”¨æˆ·å¯è®¾ç½®
  }
  
  // ç”¨æˆ·è®¾ç½®è¦†ç›–é»˜è®¤è®¾ç½®
  const finalSettings = { ...defaultSettings, ...userSettings }
  
  return this.conversationManager.createConversation(modelId, finalSettings)
}
```

### ä¸Šä¸‹æ–‡ç®¡ç†é›†æˆ

**ContextManager ä½¿ç”¨ ModelConfig**:
```typescript
class ContextManager {
  async trimMessages(
    messages: ChatMessage[],
    conversation: Conversation
  ): Promise<ChatMessage[]> {
    const modelConfig = await this.getModelConfig(conversation.modelId)
    
    // ä½¿ç”¨æ¨¡å‹çš„å®é™…ä¸Šä¸‹æ–‡é•¿åº¦
    const maxContextTokens = modelConfig.contextLength
    
    // ä¸ºç”Ÿæˆé¢„ç•™ç©ºé—´
    const reservedTokens = Math.min(
      conversation.settings.maxTokens,
      modelConfig.maxTokens
    )
    
    const availableTokens = maxContextTokens - reservedTokens
    
    return this.smartTrim(messages, availableTokens)
  }
}
```

### LangChain å®¢æˆ·ç«¯é…ç½®

**æ ¹æ® ModelConfig é…ç½® LangChain**:
```typescript
private async createLangChainClient(modelId: string): Promise<LangChainClient> {
  const modelConfig = await this.getModelConfig(modelId)
  const [providerId, modelName] = modelId.split('.')
  const provider = await this.llmConfigManager.getProvider(providerId)
  
  return new LangChainClient({
    modelName,
    apiKey: provider.apiKey,
    baseUrl: provider.baseUrl,
    temperature: 0.7,                    // å¯¹è¯æ—¶å¯è°ƒæ•´
    maxTokens: modelConfig.maxTokens,    // ä»é…ç½®ç»§æ‰¿
    timeout: modelConfig.timeout,        // ä»é…ç½®ç»§æ‰¿
    maxRetries: modelConfig.maxRetries,  // ä»é…ç½®ç»§æ‰¿
    
    // æ ¹æ® completionMode é€‰æ‹© API ç±»å‹
    useChat: modelConfig.completionMode === 'å¯¹è¯'
  })
}
```

### å®æ—¶é…ç½®æ›´æ–°

**ç›‘å¬æ¨¡å‹é…ç½®å˜æ›´**:
```typescript
class LlmChatService {
  async initialize() {
    // ç›‘å¬æ¨¡å‹é…ç½®å˜æ›´
    this.llmConfigManager.onProviderUpdated((providerId) => {
      // æ¸…ç†ç›¸å…³çš„å®¢æˆ·ç«¯ç¼“å­˜
      for (const [modelId, client] of this.activeClients.entries()) {
        if (modelId.startsWith(`${providerId}.`)) {
          this.activeClients.delete(modelId)
        }
      }
    })
  }
}
```

---

## ğŸ”— IPC é€šä¿¡åè®®

### å¯¹è¯ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm-chat:create-conversation` | `{ modelId: string; settings?: Partial<ConversationSettings> }` | `{ success: boolean; conversation?: Conversation }` | åˆ›å»ºæ–°å¯¹è¯ |
| `llm-chat:get-conversations` | `undefined` | `{ success: boolean; conversations?: Conversation[] }` | è·å–æ‰€æœ‰å¯¹è¯ |
| `llm-chat:get-conversation` | `{ conversationId: string }` | `{ success: boolean; conversation?: Conversation }` | è·å–å•ä¸ªå¯¹è¯ |
| `llm-chat:delete-conversation` | `{ conversationId: string }` | `{ success: boolean }` | åˆ é™¤å¯¹è¯ |
| `llm-chat:update-title` | `{ conversationId: string; title: string }` | `{ success: boolean }` | æ›´æ–°å¯¹è¯æ ‡é¢˜ |
| `llm-chat:update-settings` | `{ conversationId: string; settings: Partial<ConversationSettings> }` | `{ success: boolean }` | æ›´æ–°å¯¹è¯è®¾ç½® |

### æ¶ˆæ¯ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm-chat:send-message` | `{ conversationId: string; content: string; fileReferences?: FileReference[] }` | `{ success: boolean; messageId?: string }` | å‘é€æ¶ˆæ¯ |
| `llm-chat:stream-chunk` | - | `{ conversationId: string; messageId: string; chunk: string }` | æµå¼å“åº”å—ï¼ˆä¸»è¿›ç¨‹ â†’ æ¸²æŸ“è¿›ç¨‹ï¼‰ |
| `llm-chat:stream-complete` | - | `{ conversationId: string; messageId: string }` | æµå¼å“åº”å®Œæˆ |
| `llm-chat:stream-error` | - | `{ conversationId: string; error: string }` | æµå¼å“åº”é”™è¯¯ |
| `llm-chat:regenerate-message` | `{ conversationId: string; messageId: string }` | `{ success: boolean }` | é‡æ–°ç”Ÿæˆæ¶ˆæ¯ |
| `llm-chat:delete-message` | `{ conversationId: string; messageId: string }` | `{ success: boolean }` | åˆ é™¤æ¶ˆæ¯ |

### æ¨¡å‹ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm-chat:get-available-models` | `undefined` | `{ success: boolean; models?: ActiveModel[] }` | è·å–å¯ç”¨æ¨¡å‹ |
| `llm-chat:switch-model` | `{ conversationId: string; modelId: string }` | `{ success: boolean }` | åˆ‡æ¢å¯¹è¯æ¨¡å‹ |

---

## ğŸ“Š å‰ç«¯ Store è®¾è®¡

### llmChatStore.ts çŠ¶æ€ç»“æ„

```typescript
interface ChatState {
  // å¯¹è¯åˆ—è¡¨
  conversations: Conversation[]
  
  // å½“å‰æ´»è·ƒå¯¹è¯ID
  activeConversationId: string | null
  
  // åŠ è½½çŠ¶æ€
  isLoading: boolean
  isSending: boolean
  
  // é€‰ä¸­çš„æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨æˆ·åœ¨ ModelSelector ä¸­é€‰æ‹©çš„ï¼‰
  selectedModels: string[]  // modelId æ•°ç»„
  
  // é»˜è®¤æ¨¡å‹
  defaultModel: string | null
  
  // æµå¼å“åº”ä¸´æ—¶å†…å®¹
  streamingContent: {
    conversationId: string
    messageId: string
    content: string
  } | null
  
  // é”™è¯¯ä¿¡æ¯
  error: string | null
}
```

### llmChatStore.ts æ ¸å¿ƒæ–¹æ³•

```typescript
export const useLlmChatStore = defineStore('llmChat', () => {
  // çŠ¶æ€
  const conversations = ref<Conversation[]>([])
  const activeConversationId = ref<string | null>(null)
  const isLoading = ref(false)
  const isSending = ref(false)
  const selectedModels = ref<string[]>([])
  const defaultModel = ref<string | null>(null)
  const streamingContent = ref<StreamingContent | null>(null)
  const error = ref<string | null>(null)
  
  // è®¡ç®—å±æ€§
  const activeConversation = computed(() => 
    conversations.value.find(c => c.id === activeConversationId.value)
  )
  
  const sortedConversations = computed(() => 
    [...conversations.value].sort((a, b) => 
      b.updatedAt.getTime() - a.updatedAt.getTime()
    )
  )
  
  // åˆå§‹åŒ–
  async function initialize() {
    await loadConversations()
    await loadSelectedModels()
    setupStreamListeners()
  }
  
  // åŠ è½½å¯¹è¯åˆ—è¡¨
  async function loadConversations() {
    const result = await window.api.llmChat.getConversations()
    if (result.success) {
      conversations.value = result.conversations || []
    }
  }
  
  // åˆ›å»ºæ–°å¯¹è¯
  async function createConversation(modelId?: string) {
    const model = modelId || defaultModel.value || selectedModels.value[0]
    if (!model) {
      error.value = 'è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹'
      return null
    }
    
    const result = await window.api.llmChat.createConversation({ modelId: model })
    if (result.success && result.conversation) {
      conversations.value.push(result.conversation)
      activeConversationId.value = result.conversation.id
      return result.conversation
    }
    return null
  }
  
  // å‘é€æ¶ˆæ¯
  async function sendMessage(content: string) {
    if (!activeConversationId.value) {
      error.value = 'æ²¡æœ‰æ´»è·ƒçš„å¯¹è¯'
      return
    }
    
    isSending.value = true
    error.value = null
    
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°æœ¬åœ°
    const userMessage: ChatMessage = {
      id: nanoid(),
      role: 'user',
      content,
      timestamp: new Date()
    }
    
    const conversation = activeConversation.value
    if (conversation) {
      conversation.messages.push(userMessage)
    }
    
    // åˆ›å»ºå ä½ AI æ¶ˆæ¯
    const aiMessage: ChatMessage = {
      id: nanoid(),
      role: 'assistant',
      content: '',
      timestamp: new Date()
    }
    conversation?.messages.push(aiMessage)
    
    // åˆå§‹åŒ–æµå¼å†…å®¹
    streamingContent.value = {
      conversationId: activeConversationId.value,
      messageId: aiMessage.id,
      content: ''
    }
    
    // å‘é€åˆ°ä¸»è¿›ç¨‹
    const result = await window.api.llmChat.sendMessage({
      conversationId: activeConversationId.value,
      content
    })
    
    if (!result.success) {
      error.value = result.error || 'å‘é€å¤±è´¥'
      isSending.value = false
    }
  }
  
  // è®¾ç½®æµå¼ç›‘å¬å™¨
  function setupStreamListeners() {
    // æ¥æ”¶æµå¼å—
    window.api.llmChat.onStreamChunk((data) => {
      if (streamingContent.value?.messageId === data.messageId) {
        streamingContent.value.content += data.chunk
        
        // æ›´æ–°å¯¹è¯ä¸­çš„æ¶ˆæ¯
        const conversation = conversations.value.find(c => c.id === data.conversationId)
        const message = conversation?.messages.find(m => m.id === data.messageId)
        if (message) {
          message.content = streamingContent.value.content
        }
      }
    })
    
    // æµå¼å®Œæˆ
    window.api.llmChat.onStreamComplete((data) => {
      if (streamingContent.value?.messageId === data.messageId) {
        streamingContent.value = null
        isSending.value = false
        
        // æ›´æ–°å¯¹è¯æ—¶é—´
        const conversation = conversations.value.find(c => c.id === data.conversationId)
        if (conversation) {
          conversation.updatedAt = new Date()
        }
      }
    })
    
    // æµå¼é”™è¯¯
    window.api.llmChat.onStreamError((data) => {
      error.value = data.error
      streamingContent.value = null
      isSending.value = false
    })
  }
  
  // åˆ é™¤å¯¹è¯
  async function deleteConversation(conversationId: string) {
    const result = await window.api.llmChat.deleteConversation({ conversationId })
    if (result.success) {
      conversations.value = conversations.value.filter(c => c.id !== conversationId)
      if (activeConversationId.value === conversationId) {
        activeConversationId.value = null
      }
    }
  }
  
  // åˆ‡æ¢æ´»è·ƒå¯¹è¯
  function setActiveConversation(conversationId: string) {
    activeConversationId.value = conversationId
  }
  
  // åŠ è½½é€‰ä¸­çš„æ¨¡å‹
  async function loadSelectedModels() {
    const stored = localStorage.getItem('nimbria_llm_chat_selected_models')
    if (stored) {
      selectedModels.value = JSON.parse(stored)
    }
  }
  
  // ä¿å­˜é€‰ä¸­çš„æ¨¡å‹
  function saveSelectedModels() {
    localStorage.setItem('nimbria_llm_chat_selected_models', JSON.stringify(selectedModels.value))
  }
  
  // è®¾ç½®é€‰ä¸­çš„æ¨¡å‹
  function setSelectedModels(models: string[]) {
    selectedModels.value = models
    saveSelectedModels()
    
    // è®¾ç½®é»˜è®¤æ¨¡å‹ä¸ºç¬¬ä¸€ä¸ª
    if (models.length > 0 && !defaultModel.value) {
      defaultModel.value = models[0]
    }
  }
  
  return {
    // çŠ¶æ€
    conversations,
    activeConversationId,
    isLoading,
    isSending,
    selectedModels,
    defaultModel,
    streamingContent,
    error,
    
    // è®¡ç®—å±æ€§
    activeConversation,
    sortedConversations,
    
    // æ–¹æ³•
    initialize,
    loadConversations,
    createConversation,
    sendMessage,
    deleteConversation,
    setActiveConversation,
    setSelectedModels
  }
})
```

---

## ğŸ”„ å®Œæ•´äº¤äº’æµç¨‹

### 1. åˆå§‹åŒ–æµç¨‹

```
1. ç”¨æˆ·æ‰“å¼€é¡¹ç›®é¡µé¢
   â†“
2. LlmChatPanel ç»„ä»¶æŒ‚è½½
   â†“
3. llmChatStore.initialize()
   â†“
4. åŠ è½½å¯¹è¯åˆ—è¡¨ (ä» LocalStorage)
   â†“
5. åŠ è½½é€‰ä¸­çš„æ¨¡å‹åˆ—è¡¨ (ä» LocalStorage)
   â†“
6. è®¾ç½®æµå¼ç›‘å¬å™¨
   â†“
7. æ˜¾ç¤ºå¯¹è¯åˆ—è¡¨
```

### 2. åˆ›å»ºå¯¹è¯æµç¨‹

```
1. ç”¨æˆ·ç‚¹å‡»"æ–°å»ºå¯¹è¯"
   â†“
2. llmChatStore.createConversation()
   â†“
3. IPC: llm-chat:create-conversation
   â†“
4. ä¸»è¿›ç¨‹ LlmChatService.createConversation()
   â†“
5. ConversationManager åˆ›å»ºå¯¹è¯å¯¹è±¡
   â†“
6. ä¿å­˜åˆ° LocalStorage
   â†“
7. è¿”å›å¯¹è¯å¯¹è±¡
   â†“
8. å‰ç«¯æ·»åŠ åˆ°åˆ—è¡¨å¹¶è®¾ä¸ºæ´»è·ƒ
```

### 3. å‘é€æ¶ˆæ¯æµç¨‹ï¼ˆæµå¼ï¼‰

```
1. ç”¨æˆ·è¾“å…¥æ¶ˆæ¯å¹¶ç‚¹å‡»å‘é€
   â†“
2. llmChatStore.sendMessage(content)
   â†“
3. æœ¬åœ°æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯
   â†“
4. åˆ›å»ºå ä½ AI æ¶ˆæ¯
   â†“
5. IPC: llm-chat:send-message
   â†“
6. ä¸»è¿›ç¨‹ LlmChatService.sendMessage()
   â†“
7. å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†ï¼‰
   â†“
8. è·å– LangChainClient
   â†“
9. è°ƒç”¨ chatStream()
   â†“
10. LangChain å¼€å§‹æµå¼è¿”å›
    â†“
11. æ¯ä¸ªå—é€šè¿‡ IPC å‘é€åˆ°å‰ç«¯
    â†“ (å¾ªç¯)
12. å‰ç«¯æ¥æ”¶å—ï¼Œæ›´æ–° UI
    â†“
13. æµå¼å®Œæˆï¼Œä¿å­˜å®Œæ•´æ¶ˆæ¯
    â†“
14. æ›´æ–° LocalStorage
```

---

## ğŸ“¦ ä¾èµ–åŒ…

### éœ€è¦å®‰è£…çš„ npm åŒ…

```json
{
  "dependencies": {
    "langchain": "^0.1.0",
    "@langchain/openai": "^0.0.19",
    "@langchain/core": "^0.1.0",
    "tiktoken": "^1.0.10"
  }
}
```

---

## ğŸš€ å®ç°ä¼˜å…ˆçº§

### Phase 1: åŸºç¡€å¯¹è¯åŠŸèƒ½ (æ ¸å¿ƒ)
- âœ… LangChainClient åŸºç¡€å°è£…
- âœ… ConversationManager å¯¹è¯ç®¡ç†
- âœ… LlmChatService ä¸»æœåŠ¡
- âœ… IPC é€šä¿¡åè®®
- âœ… å‰ç«¯ chatStore
- âœ… æµå¼å“åº”

### Phase 2: ä¸Šä¸‹æ–‡ç®¡ç†
- â³ ContextManager å®ç°
- â³ Token è®¡æ•°
- â³ æ™ºèƒ½è£å‰ª

### Phase 3: ä½“éªŒä¼˜åŒ–
- â³ é‡æ–°ç”Ÿæˆæ¶ˆæ¯
- â³ å¯¹è¯å¯¼å‡º
- â³ æœç´¢å†å²æ¶ˆæ¯
- â³ å¯¹è¯æ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆ

### Phase 4: é«˜çº§åŠŸèƒ½
- â³ é”™è¯¯é‡è¯•æœºåˆ¶
- â³ æ€§èƒ½ä¼˜åŒ–
- â³ ä¸Šä¸‹æ–‡ä½¿ç”¨å¯è§†åŒ–
- â³ å¯¹è¯è®¾ç½®é¢„è®¾

---

## ğŸ› é”™è¯¯å¤„ç†ç­–ç•¥

### å¸¸è§é”™è¯¯ç±»å‹

| é”™è¯¯ç±»å‹ | å¤„ç†ç­–ç•¥ |
|---------|---------|
| ç½‘ç»œé”™è¯¯ | è‡ªåŠ¨é‡è¯• 3 æ¬¡ï¼ŒæŒ‡æ•°é€€é¿ |
| API å¯†é’¥æ— æ•ˆ | æç¤ºç”¨æˆ·æ£€æŸ¥é…ç½® |
| Token è¶…é™ | è‡ªåŠ¨è£å‰ªä¸Šä¸‹æ–‡ |
| æ¨¡å‹ä¸å¯ç”¨ | æç¤ºåˆ‡æ¢æ¨¡å‹ |
| æµå¼ä¸­æ–­ | ä¿å­˜å·²æ¥æ”¶å†…å®¹ï¼Œæ ‡è®°ä¸ºæœªå®Œæˆ |

### é”™è¯¯æ¢å¤æœºåˆ¶

```typescript
// è‡ªåŠ¨é‡è¯•
async function sendMessageWithRetry(
  conversationId: string,
  content: string,
  maxRetries: number = 3
) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      await sendMessage(conversationId, content)
      return
    } catch (error) {
      if (i === maxRetries - 1) throw error
      await delay(Math.pow(2, i) * 1000) // æŒ‡æ•°é€€é¿
    }
  }
}
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. æ¶ˆæ¯åŠ è½½ä¼˜åŒ–
- åˆ†é¡µåŠ è½½å†å²æ¶ˆæ¯
- è™šæ‹Ÿæ»šåŠ¨é•¿å¯¹è¯åˆ—è¡¨

### 2. Token è®¡æ•°ç¼“å­˜
- ç¼“å­˜å·²è®¡ç®—çš„ Token æ•°
- é¿å…é‡å¤è®¡ç®—

### 3. æµå¼å“åº”ä¼˜åŒ–
- æ‰¹é‡æ›´æ–° UIï¼ˆé˜²æŠ–ï¼‰
- ä½¿ç”¨ Web Worker å¤„ç†å¤§é‡æ–‡æœ¬

### 4. LocalStorage ä¼˜åŒ–
- å‹ç¼©å­˜å‚¨æ•°æ®
- å®šæœŸæ¸…ç†æ—§å¯¹è¯
- é™åˆ¶å•ä¸ªå¯¹è¯çš„æ¶ˆæ¯æ•°é‡

---

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [AIæ¨¡å‹æœåŠ¡ä¸è°ƒç”¨ç³»ç»Ÿè®¾è®¡æ–‡æ¡£](../Ai æ¨¡å‹æœåŠ¡ä¸“é¡¹/AIæ¨¡å‹æœåŠ¡ä¸è°ƒç”¨ç³»ç»Ÿè®¾è®¡æ–‡æ¡£.md)
- [LLMé…ç½®ç³»ç»Ÿè®¾è®¡æ–‡æ¡£](../Ai æ¨¡å‹æœåŠ¡ä¸“é¡¹/LLMé…ç½®ç³»ç»Ÿè®¾è®¡æ–‡æ¡£.md)
- [LLMå¯¹è¯ç³»ç»Ÿäº¤äº’è®¾è®¡](./Design.md)

---

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ15æ—¥  
**è´Ÿè´£äºº**: Nimbria å¼€å‘å›¢é˜Ÿ

