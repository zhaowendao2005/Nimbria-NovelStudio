# Nimbria AI æ¨¡å‹æœåŠ¡ä¸è°ƒç”¨ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2025å¹´10æœˆ14æ—¥  
**æ–‡æ¡£çŠ¶æ€**: åæ˜ å®é™…å®ç°  

---

## ğŸ“‹ ç³»ç»Ÿæ¦‚è¿°

Nimbria çš„ AI æ¨¡å‹æœåŠ¡ä¸è°ƒç”¨ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ã€ç±»å‹å®‰å…¨çš„å¤§è¯­è¨€æ¨¡å‹äº¤äº’å¹³å°ã€‚ç³»ç»Ÿæ”¯æŒå¤šæä¾›å•†ç®¡ç†ã€æ´»è·ƒæ¨¡å‹é…ç½®ã€å®æ—¶å¯¹è¯äº¤äº’ï¼Œå¹¶ä¸ºå…¶ä»–ä¸šåŠ¡æ¨¡å—æä¾›ç®€æ´çš„ API æ¥å£ã€‚é€šè¿‡ Electron ä¸»è¿›ç¨‹çš„å®‰å…¨éš”ç¦»ï¼Œç¡®ä¿ API å¯†é’¥å’Œæ¨¡å‹è°ƒç”¨çš„å®‰å…¨æ€§ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **å¤šæä¾›å•†æ”¯æŒ**: ç»Ÿä¸€ç®¡ç† OpenAIã€Anthropicã€è‡ªå®šä¹‰æä¾›å•†ç­‰å¤šç§ LLM æœåŠ¡
- **æ´»è·ƒæ¨¡å‹é…ç½®**: ç”¨æˆ·å¯é€‰æ‹©å’Œé…ç½®ä¸åŒç±»å‹çš„æ´»è·ƒæ¨¡å‹ï¼ˆLLMã€æ–‡æœ¬åµŒå…¥ã€å›¾åƒç”Ÿæˆç­‰ï¼‰
- **é…ç½®æŒä¹…åŒ–**: æ‰€æœ‰é…ç½®è‡ªåŠ¨ä¿å­˜åˆ° YAML æ–‡ä»¶ï¼Œæ”¯æŒå¯¼å…¥å¯¼å‡º
- **ç±»å‹å®‰å…¨è°ƒç”¨**: å®Œæ•´çš„ TypeScript æ”¯æŒï¼Œç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
- **å®æ—¶å¯¹è¯ç³»ç»Ÿ**: å®Œæ•´çš„ LLM Chat åŠŸèƒ½ï¼Œæ”¯æŒæµå¼å“åº”å’Œå¯¹è¯ç®¡ç†
- **æ•°æ®åº“æŒä¹…åŒ–**: å¯¹è¯å†å²å­˜å‚¨åœ¨é¡¹ç›®çº§ SQLite æ•°æ®åº“ä¸­
- **äº‹ä»¶é©±åŠ¨æ¶æ„**: ä½¿ç”¨ EventEmitter å®ç°æ¾è€¦åˆçš„æ¶ˆæ¯ä¼ é€’
- **è¿æ¥æµ‹è¯•ä¸æ¨¡å‹å‘ç°**: è‡ªåŠ¨æµ‹è¯•è¿æ¥å¹¶å‘ç°å¯ç”¨æ¨¡å‹
- **é”™è¯¯å¤„ç†ä¸é‡è¯•**: å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶å’Œè‡ªåŠ¨é‡è¯•é€»è¾‘

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ•°æ®é“¾è·¯

```mermaid
graph TD
    A[ä¸šåŠ¡æ¨¡å—<br/>å¦‚ Markdown ç¼–è¾‘å™¨] -->|è°ƒç”¨å…¨å±€ LLM API| B[LLM Service Store<br/>å‰ç«¯ç»Ÿä¸€æ¥å£]
    B -->|IPC è°ƒç”¨| C[ä¸»è¿›ç¨‹ LLM Handlers]
    C --> D{æ“ä½œç±»å‹}
    D -->|é…ç½®ç®¡ç†| E[LlmConfigManager<br/>YAML é…ç½®æ–‡ä»¶]
    D -->|å¯¹è¯ç®¡ç†| F[LlmChatService<br/>å¯¹è¯åè°ƒå™¨]
    F --> G[ConversationManager<br/>å¯¹è¯æ•°æ®ç®¡ç†]
    F --> H[LangChainClient<br/>API è°ƒç”¨å°è£…]
    F --> I[ContextManager<br/>ä¸Šä¸‹æ–‡ç®¡ç†]
    G -->|è¯»å†™å¯¹è¯| J[ProjectDatabase<br/>SQLite æ•°æ®åº“]
    H -->|HTTP è¯·æ±‚| K[å¤–éƒ¨ LLM æä¾›å•†<br/>OpenAI/Anthropic/Custom]
    E -->|è¯»å†™é…ç½®| L[providers.yaml<br/>ç”¨æˆ·æ•°æ®ç›®å½•]
    
    subgraph Frontend[å‰ç«¯æ¸²æŸ“è¿›ç¨‹]
        A
        B
        M[LLM Chat UI<br/>å¯¹è¯ç•Œé¢]
        N[Settings UI<br/>æ¨¡å‹æœåŠ¡é…ç½®é¡µé¢]
    end
    
    subgraph MainProcess[ä¸»è¿›ç¨‹ Electron]
        C
        E
        F
        G
        H
        I
    end
    
    M -->|å¯¹è¯æ“ä½œ| B
    N -->|é…ç½®ç®¡ç†| B
    K -->|æµå¼å“åº”| H
    F -.->|äº‹ä»¶å¹¿æ’­| C
    C -.->|äº‹ä»¶è½¬å‘| M
```

### æ–‡ä»¶æ¶æ„

```
Nimbria/
â”œâ”€â”€ Client/                                    # å‰ç«¯ä»£ç 
â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â”œâ”€â”€ settings/                          # é…ç½®ç®¡ç† Store
â”‚   â”‚   â”‚   â”œâ”€â”€ settings.llm.store.ts          # LLM é…ç½®çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ DataSource.ts                  # æ•°æ®æºæŠ½è±¡å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts                       # å‰ç«¯ç±»å‹å®šä¹‰
â”‚   â”‚   â”‚   â””â”€â”€ llm.mock.ts                    # Mock æ•°æ®ï¼ˆå·²åºŸå¼ƒï¼‰
â”‚   â”‚   â””â”€â”€ llm/                               # å…¨å±€ LLM æœåŠ¡ Store
â”‚   â”‚       â”œâ”€â”€ llm.service.store.ts           # å…¨å±€ LLM è°ƒç”¨æ¥å£
â”‚   â”‚       â””â”€â”€ types.ts                       # æœåŠ¡å±‚ç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ GUI/components/HomeDashboardPage/Settings/  # é…ç½® UI ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ Settings.LlmConfig.vue             # ä¸»é…ç½®é¡µé¢
â”‚   â”‚   â”œâ”€â”€ Settings.LlmConfig.ProviderList.vue    # æä¾›å•†åˆ—è¡¨
â”‚   â”‚   â”œâ”€â”€ Settings.LlmConfig.ActiveModels.vue    # æ´»è·ƒæ¨¡å‹ç®¡ç†
â”‚   â”‚   â””â”€â”€ Settings.LlmConfig.*.vue           # å…¶ä»–é…ç½®ç»„ä»¶
â”‚   â””â”€â”€ types/core/window.d.ts                 # å…¨å±€ API ç±»å‹å®šä¹‰
â”œâ”€â”€ src-electron/                              # ä¸»è¿›ç¨‹ä»£ç 
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm-service/                       # LLM é…ç½®æœåŠ¡å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ llm-config-manager.ts          # é…ç½®ç®¡ç†å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ llm-api-client.ts              # API å®¢æˆ·ç«¯å°è£…
â”‚   â”‚   â”‚   â””â”€â”€ types.ts                       # é…ç½®ç±»å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ llm-chat-service/                  # LLM å¯¹è¯æœåŠ¡å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ llm-chat-service.ts            # ä¸»æœåŠ¡åè°ƒå™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation-manager.ts        # å¯¹è¯æ•°æ®ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ langchain-client.ts            # LangChain API å°è£…
â”‚   â”‚   â”‚   â”œâ”€â”€ context-manager.ts             # ä¸Šä¸‹æ–‡ç®¡ç†
â”‚   â”‚   â”‚   â””â”€â”€ types.ts                       # å¯¹è¯ç±»å‹å®šä¹‰
â”‚   â”‚   â””â”€â”€ database-service/                  # æ•°æ®åº“æœåŠ¡
â”‚   â”‚       â””â”€â”€ project-database.ts            # é¡¹ç›®æ•°æ®åº“æ“ä½œ
â”‚   â”œâ”€â”€ ipc/main-renderer/
â”‚   â”‚   â”œâ”€â”€ llm-handlers.ts                    # LLM é…ç½® IPC å¤„ç†å™¨
â”‚   â”‚   â””â”€â”€ llm-chat-handlers.ts               # LLM å¯¹è¯ IPC å¤„ç†å™¨
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ main-preload.ts                    # API æš´éœ²
â”‚       â”œâ”€â”€ project-preload.ts                 # é¡¹ç›®çª—å£ API æš´éœ²
â”‚       â””â”€â”€ app-manager.ts                     # æœåŠ¡æ³¨å†Œ
â””â”€â”€ AppData/llm-config/                        # é…ç½®æ–‡ä»¶å­˜å‚¨
    â””â”€â”€ providers.yaml                         # æä¾›å•†é…ç½®æ–‡ä»¶
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. LlmConfigManager (é…ç½®ç®¡ç†)

**èŒè´£**: ç®¡ç†æä¾›å•†é…ç½®çš„ YAML æ–‡ä»¶è¯»å†™å’ŒæŒä¹…åŒ–

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class LlmConfigManager {
  // é…ç½®æ–‡ä»¶ç®¡ç†
  async loadProviders(): Promise<ModelProvider[]>
  async saveProviders(providers: ModelProvider[]): Promise<void>
  
  // æä¾›å•†ç®¡ç†
  async addProvider(provider: ModelProvider): Promise<ModelProvider>
  async updateProvider(providerId: string, updates: Partial<ModelProvider>): Promise<ModelProvider>
  async removeProvider(providerId: string): Promise<void>
  async getProvider(providerId: string): Promise<ModelProvider | null>
}
```

**é…ç½®æ–‡ä»¶ç»“æ„**:
```yaml
providers:
  - id: openai
    name: openai
    displayName: OpenAI
    status: active
    apiKey: sk-xxxxx
    baseUrl: https://api.openai.com/v1
    activeModels:
      LLM:
        selectedModels: [gpt-4o, gpt-4]
        preferredModel: gpt-4o
      TEXT_EMBEDDING:
        selectedModels: [text-embedding-3-large]
        preferredModel: text-embedding-3-large
```

### 2. LlmChatService (å¯¹è¯æœåŠ¡åè°ƒå™¨)

**èŒè´£**: åè°ƒå„ä¸ªç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€çš„å¯¹è¯æœåŠ¡æ¥å£ï¼Œä½¿ç”¨ EventEmitter å®ç°äº‹ä»¶é©±åŠ¨æ¶æ„

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class LlmChatService extends EventEmitter {
  // æœåŠ¡ç®¡ç†
  async initialize(projectPath?: string): Promise<void>
  async switchProject(projectPath: string): Promise<void>
  
  // å¯¹è¯ç®¡ç†
  async createConversation(modelId: string, settings?: Partial<ConversationSettings>): Promise<string>
  async sendMessage(conversationId: string, content: string): Promise<string>
  async regenerateMessage(conversationId: string): Promise<void>
  async deleteMessage(conversationId: string, messageId: string): Promise<void>
  
  // æ•°æ®è®¿é—®
  getConversations(): Conversation[]
  getConversation(conversationId: string): Conversation | null
  async updateConversationTitle(conversationId: string, title: string): Promise<void>
  async deleteConversation(conversationId: string): Promise<void>
}
```

### 3. ConversationManager (å¯¹è¯æ•°æ®ç®¡ç†)

**èŒè´£**: ç®¡ç†å¯¹è¯çš„åˆ›å»ºã€åˆ é™¤ã€å†å²è®°å½•ï¼Œæ•°æ®å­˜å‚¨åœ¨é¡¹ç›®æ•°æ®åº“ä¸­

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class ConversationManager {
  // æ•°æ®åº“ç®¡ç†
  setProjectDatabase(projectDatabase: ProjectDatabase): void
  async initialize(): Promise<void>
  
  // å¯¹è¯æ“ä½œ
  async createConversation(conversationId: string, modelId: string, settings: ConversationSettings): Promise<Conversation>
  async addMessage(conversationId: string, message: ChatMessage): Promise<ChatMessage>
  async updateConversationTitle(conversationId: string, title: string): Promise<void>
  async deleteConversation(conversationId: string): Promise<void>
  async deleteMessage(conversationId: string, messageId: string): Promise<void>
}
```

### 4. LangChainClient (API è°ƒç”¨å°è£…)

**èŒè´£**: å°è£… LangChain çš„ ChatOpenAIï¼Œæä¾›æµå¼å’Œéæµå¼èŠå¤©åŠŸèƒ½

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class LangChainClient {
  // èŠå¤©è°ƒç”¨
  async chatStream(messages: ChatMessage[], callbacks: StreamCallbacks): Promise<void>
  async chat(messages: ChatMessage[]): Promise<string>
  
  // å·¥å…·æ–¹æ³•
  countTokens(messages: ChatMessage[]): number
  private convertMessages(messages: ChatMessage[]): BaseMessage[]
}
```

### 5. LLM Chat Store (å‰ç«¯å¯¹è¯çŠ¶æ€ç®¡ç†)

**èŒè´£**: ç®¡ç†å‰ç«¯çš„å¯¹è¯çŠ¶æ€ï¼Œä¸åç«¯ LlmChatService é€šä¿¡ï¼Œå¤„ç†æµå¼å“åº”

**æ ¸å¿ƒçŠ¶æ€**:
```typescript
export const useLlmChatStore = defineStore('llmChat', {
  state: () => ({
    // å¯¹è¯æ•°æ®
    conversations: Conversation[],
    activeConversationId: string | null,
    
    // åŠ è½½çŠ¶æ€
    isLoading: boolean,
    isSending: boolean,
    
    // æµå¼å“åº”çŠ¶æ€
    streamingMessageId: string | null,
    streamingContent: string,
    
    // æ¨¡å‹ç®¡ç†
    selectedModels: string[]
  }),
  
  // æ ¸å¿ƒæ“ä½œ
  async initialize(): Promise<void>
  async createConversation(modelId?: string): Promise<string | null>
  async sendMessage(content: string): Promise<void>
  async updateConversationTitle(conversationId: string, title: string): Promise<void>
  async deleteConversation(conversationId: string): Promise<void>
})
```

### 6. Settings LLM Store (å‰ç«¯é…ç½®çŠ¶æ€ç®¡ç†)

**èŒè´£**: ç®¡ç†å‰ç«¯çš„ LLM é…ç½®çŠ¶æ€ï¼Œæä¾›å“åº”å¼æ•°æ®å’Œæ“ä½œæ–¹æ³•

**æ ¸å¿ƒçŠ¶æ€**:
```typescript
export const useSettingsLlmStore = defineStore('settings-llm', () => {
  // åŸºç¡€çŠ¶æ€
  const providers = ref<ModelProvider[]>([])
  const loading = ref(false)
  const error = ref<string | null>(null)
  
  // è®¡ç®—å±æ€§
  const activeProviders = computed(() => providers.value.filter(p => p.status === 'active'))
  const activeModelTypes = computed(() => /* è·å–æ‰€æœ‰æ´»è·ƒæ¨¡å‹ç±»å‹ */)
  
  // æ“ä½œæ–¹æ³•
  async function initialize(): Promise<void>
  async function addProvider(provider: Omit<ModelProvider, 'id'>): Promise<boolean>
  async function toggleModelSelection(providerId: string, modelType: string, modelName: string): Promise<boolean>
  async function setPreferredModel(providerId: string, modelType: string, modelName: string): Promise<boolean>
})
```

### 7. å…¨å±€ LLM æœåŠ¡ Store (å¾…å®ç°)

**èŒè´£**: ä¸ºå…¶ä»–ä¸šåŠ¡æ¨¡å—æä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£

```typescript
export const useLlmServiceStore = defineStore('llm-service', () => {
  // è·å–å¯ç”¨æ¨¡å‹
  function getActiveModels(modelType: ModelType): ActiveModel[]
  function getPreferredModel(modelType: ModelType): ActiveModel | null
  
  // æ¨¡å‹è°ƒç”¨
  async function chat(options: ChatOptions): Promise<ChatResponse>
  async function chatStream(options: ChatStreamOptions): Promise<ReadableStream>
  async function embed(text: string, modelId?: string): Promise<EmbeddingResponse>
  
  // æ¨¡å‹é€‰æ‹©
  async function selectModel(modelType: ModelType): Promise<ActiveModel | null>
})
```

---

## ğŸ”— IPC é€šä¿¡åè®®

### é…ç½®ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm:get-providers` | `undefined` | `{ success: boolean; providers?: ModelProvider[] }` | è·å–æ‰€æœ‰æä¾›å•† |
| `llm:add-provider` | `{ provider: Omit<ModelProvider, 'id'> }` | `{ success: boolean; provider?: ModelProvider }` | æ·»åŠ æ–°æä¾›å•† |
| `llm:remove-provider` | `{ providerId: string }` | `{ success: boolean }` | åˆ é™¤æä¾›å•† |
| `llm:activate-provider` | `{ providerId: string }` | `{ success: boolean; provider?: ModelProvider }` | æ¿€æ´»æä¾›å•† |
| `llm:refresh-models` | `{ providerId: string }` | `{ success: boolean; modelsCount?: number }` | åˆ·æ–°æ¨¡å‹åˆ—è¡¨ |
| `llm:test-connection` | `{ providerId: string }` | `{ success: boolean; message?: string }` | æµ‹è¯•è¿æ¥ |

### æ´»è·ƒæ¨¡å‹ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm:toggle-model-selection` | `{ providerId: string; modelType: string; modelName: string }` | `{ success: boolean }` | åˆ‡æ¢æ¨¡å‹é€‰æ‹©çŠ¶æ€ |
| `llm:set-preferred-model` | `{ providerId: string; modelType: string; modelName: string }` | `{ success: boolean }` | è®¾ç½®é¦–é€‰æ¨¡å‹ |

### LLM Chat å¯¹è¯ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm-chat:create-conversation` | `{ modelId: string; settings?: Partial<ConversationSettings> }` | `{ success: boolean; conversationId?: string }` | åˆ›å»ºæ–°å¯¹è¯ |
| `llm-chat:get-conversations` | `undefined` | `{ success: boolean; conversations?: Conversation[] }` | è·å–æ‰€æœ‰å¯¹è¯ |
| `llm-chat:get-conversation` | `{ conversationId: string }` | `{ success: boolean; conversation?: Conversation }` | è·å–å•ä¸ªå¯¹è¯ |
| `llm-chat:send-message` | `{ conversationId: string; content: string }` | `{ success: boolean; messageId?: string }` | å‘é€æ¶ˆæ¯ |
| `llm-chat:update-title` | `{ conversationId: string; title: string }` | `{ success: boolean }` | æ›´æ–°å¯¹è¯æ ‡é¢˜ |
| `llm-chat:delete-conversation` | `{ conversationId: string }` | `{ success: boolean }` | åˆ é™¤å¯¹è¯ |
| `llm-chat:delete-message` | `{ conversationId: string; messageId: string }` | `{ success: boolean }` | åˆ é™¤æ¶ˆæ¯ |
| `llm-chat:regenerate-message` | `{ conversationId: string }` | `{ success: boolean }` | é‡æ–°ç”Ÿæˆæ¶ˆæ¯ |

### LLM Chat äº‹ä»¶å¹¿æ’­

| äº‹ä»¶å | æ•°æ®ç±»å‹ | ç”¨é€” |
|-------|---------|------|
| `llm-chat:conversation-start` | `{ conversationId: string; modelId: string }` | å¯¹è¯åˆ›å»ºå¼€å§‹ |
| `llm-chat:conversation-created` | `{ conversationId: string; conversation: Conversation }` | å¯¹è¯åˆ›å»ºå®Œæˆ |
| `llm-chat:conversation-error` | `{ conversationId: string; error: string }` | å¯¹è¯åˆ›å»ºå¤±è´¥ |
| `llm-chat:message-start` | `{ conversationId: string; messageId: string }` | æ¶ˆæ¯ç”Ÿæˆå¼€å§‹ |
| `llm-chat:stream-chunk` | `{ conversationId: string; messageId: string; chunk: string }` | æµå¼å“åº”ç‰‡æ®µ |
| `llm-chat:stream-complete` | `{ conversationId: string; messageId: string }` | æµå¼å“åº”å®Œæˆ |
| `llm-chat:stream-error` | `{ conversationId: string; messageId: string; error: string }` | æµå¼å“åº”é”™è¯¯ |

### å…¨å±€æ¨¡å‹è°ƒç”¨ IPC (å¾…å®ç°)

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm:get-active-models` | `{ modelType: ModelType }` | `{ success: boolean; models?: ActiveModel[] }` | è·å–æ´»è·ƒæ¨¡å‹åˆ—è¡¨ |
| `llm:chat` | `{ modelId: string; messages: ChatMessage[] }` | `{ success: boolean; response?: ChatResponse }` | å‘é€èŠå¤©æ¶ˆæ¯ |
| `llm:chat-stream` | `{ modelId: string; messages: ChatMessage[] }` | `{ success: boolean; streamId?: string }` | æµå¼èŠå¤© |

---

## ğŸ“Š çŠ¶æ€ç®¡ç†æ¶æ„

### æ•°æ®æµå‘

```mermaid
graph LR
    A[ç”¨æˆ·æ“ä½œ] --> B[Settings UI ç»„ä»¶]
    B --> C[Settings LLM Store]
    C --> D[DataSource å±‚]
    D --> E[IPC è°ƒç”¨]
    E --> F[ä¸»è¿›ç¨‹ Handlers]
    F --> G[LlmConfigManager]
    G --> H[YAML é…ç½®æ–‡ä»¶]
    
    I[ä¸šåŠ¡æ¨¡å—] --> J[LLM Service Store]
    J --> K[IPC è°ƒç”¨]
    K --> L[LlmChatService]
    L --> M[LlmApiClient]
    M --> N[å¤–éƒ¨ API]
```

### çŠ¶æ€åŒæ­¥æœºåˆ¶

1. **é…ç½®å˜æ›´**: ç”¨æˆ·åœ¨è®¾ç½®é¡µé¢çš„æ“ä½œä¼šç«‹å³åŒæ­¥åˆ°ä¸»è¿›ç¨‹å¹¶æŒä¹…åŒ–
2. **è·¨çª—å£åŒæ­¥**: é…ç½®å˜æ›´ä¼šé€šè¿‡ IPC å¹¿æ’­åˆ°æ‰€æœ‰çª—å£
3. **é”™è¯¯å¤„ç†**: æ“ä½œå¤±è´¥æ—¶å‰ç«¯çŠ¶æ€ä¼šå›æ»šï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§

---

## ğŸš€ ä¸šåŠ¡æ¨¡å—è°ƒç”¨æŒ‡å—

### å¿«é€Ÿä¸Šæ‰‹ - è·å–æ´»è·ƒæ¨¡å‹

```typescript
// åœ¨ä»»ä½• Vue ç»„ä»¶æˆ–ä¸šåŠ¡é€»è¾‘ä¸­
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

export default {
  setup() {
    const llmService = useLlmServiceStore()
    
    // è·å–å¯ç”¨çš„ LLM æ¨¡å‹
    const availableModels = llmService.getActiveModels('LLM')
    
    // è·å–é¦–é€‰æ¨¡å‹
    const preferredModel = llmService.getPreferredModel('LLM')
    
    return { availableModels, preferredModel }
  }
}
```

### å‘èµ·èŠå¤©å¯¹è¯

```typescript
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

async function startChat() {
  const llmService = useLlmServiceStore()
  
  // è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹ï¼ˆå¦‚æœæœ‰å¤šä¸ªå¯ç”¨ï¼‰
  const selectedModel = await llmService.selectModel('LLM')
  if (!selectedModel) {
    console.warn('æ²¡æœ‰å¯ç”¨çš„ LLM æ¨¡å‹')
    return
  }
  
  // å‘é€æ¶ˆæ¯
  try {
    const response = await llmService.chat({
      modelId: selectedModel.id,
      messages: [
        { role: 'user', content: 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±' }
      ],
      options: {
        temperature: 0.7,
        maxTokens: 1000
      }
    })
    
    console.log('AI å›å¤:', response.content)
  } catch (error) {
    console.error('èŠå¤©å¤±è´¥:', error)
  }
}
```

### æµå¼å¯¹è¯

```typescript
async function streamChat() {
  const llmService = useLlmServiceStore()
  const selectedModel = await llmService.selectModel('LLM')
  
  if (!selectedModel) return
  
  try {
    const stream = await llmService.chatStream({
      modelId: selectedModel.id,
      messages: [
        { role: 'user', content: 'è¯·å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„è¯—' }
      ]
    })
    
    const reader = stream.getReader()
    let result = ''
    
    while (true) {
      const { done, value } = await reader.read()
      if (done) break
      
      result += value
      console.log('å®æ—¶å†…å®¹:', result)
    }
  } catch (error) {
    console.error('æµå¼èŠå¤©å¤±è´¥:', error)
  }
}
```

### æ–‡æœ¬åµŒå…¥

```typescript
async function getEmbedding() {
  const llmService = useLlmServiceStore()
  
  try {
    const embedding = await llmService.embed(
      'è¿™æ˜¯éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬å†…å®¹',
      'text-embedding-3-large' // å¯é€‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é¦–é€‰æ¨¡å‹
    )
    
    console.log('å‘é‡ç»´åº¦:', embedding.dimensions)
    console.log('å‘é‡æ•°æ®:', embedding.vector)
  } catch (error) {
    console.error('åµŒå…¥å¤±è´¥:', error)
  }
}
```

---

## ğŸ’¬ LLM Chat åŠŸèƒ½ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹ - åˆ›å»ºå¯¹è¯

```typescript
// åœ¨ Vue ç»„ä»¶ä¸­ä½¿ç”¨ LLM Chat
import { useLlmChatStore } from '@stores/llmChat/llmChatStore'

export default {
  setup() {
    const llmChatStore = useLlmChatStore()
    
    // åˆå§‹åŒ– Chat Store
    onMounted(async () => {
      await llmChatStore.initialize()
    })
    
    // åˆ›å»ºæ–°å¯¹è¯
    const createNewChat = async () => {
      const conversationId = await llmChatStore.createConversation('openai.gpt-4o')
      if (conversationId) {
        console.log('å¯¹è¯åˆ›å»ºæˆåŠŸ:', conversationId)
      }
    }
    
    return { createNewChat }
  }
}
```

### å‘é€æ¶ˆæ¯å’Œå¤„ç†æµå¼å“åº”

```typescript
// å‘é€æ¶ˆæ¯
const sendMessage = async (content: string) => {
  if (!llmChatStore.activeConversationId) {
    // å¦‚æœæ²¡æœ‰æ´»è·ƒå¯¹è¯ï¼Œè‡ªåŠ¨åˆ›å»º
    await llmChatStore.createConversation()
  }
  
  // å‘é€æ¶ˆæ¯ï¼Œè‡ªåŠ¨å¤„ç†æµå¼å“åº”
  await llmChatStore.sendMessage(content)
}

// ç›‘å¬æµå¼å“åº”çŠ¶æ€
const isStreaming = computed(() => llmChatStore.streamingMessageId !== null)
const streamingContent = computed(() => llmChatStore.streamingContent)
```

### å¯¹è¯ç®¡ç†

```typescript
// è·å–æ‰€æœ‰å¯¹è¯
const conversations = computed(() => llmChatStore.conversations)

// åˆ‡æ¢æ´»è·ƒå¯¹è¯
const switchConversation = (conversationId: string) => {
  llmChatStore.activeConversationId = conversationId
}

// é‡å‘½åå¯¹è¯
const renameConversation = async (conversationId: string, newTitle: string) => {
  await llmChatStore.updateConversationTitle(conversationId, newTitle)
}

// åˆ é™¤å¯¹è¯
const deleteConversation = async (conversationId: string) => {
  await llmChatStore.deleteConversation(conversationId)
}
```

### äº‹ä»¶ç›‘å¬

```typescript
// åœ¨ç»„ä»¶ä¸­ç›‘å¬å¯¹è¯äº‹ä»¶
onMounted(() => {
  // ç›‘å¬å¯¹è¯åˆ›å»ºäº‹ä»¶
  window.nimbria.llmChat.onConversationCreated((data) => {
    console.log('æ–°å¯¹è¯åˆ›å»º:', data.conversation)
  })
  
  // ç›‘å¬æµå¼å“åº”
  window.nimbria.llmChat.onStreamChunk((data) => {
    console.log('æ”¶åˆ°å“åº”ç‰‡æ®µ:', data.chunk)
  })
  
  // ç›‘å¬å“åº”å®Œæˆ
  window.nimbria.llmChat.onStreamComplete((data) => {
    console.log('å“åº”å®Œæˆ:', data.conversationId)
  })
})
```

---

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. Markdown ç¼–è¾‘å™¨ AI åŠ©æ‰‹

```typescript
// Client/stores/projectPage/Markdown/ai-assistant.ts
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

export class MarkdownAiAssistant {
  private llmService = useLlmServiceStore()
  
  async improveText(selectedText: string): Promise<string> {
    const model = await this.llmService.selectModel('LLM')
    if (!model) throw new Error('æ²¡æœ‰å¯ç”¨çš„ LLM æ¨¡å‹')
    
    const response = await this.llmService.chat({
      modelId: model.id,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘åŠ©æ‰‹ï¼Œè¯·æ”¹è¿›ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°ã€å‡†ç¡®ã€æ˜“è¯»ã€‚'
        },
        {
          role: 'user',
          content: `è¯·æ”¹è¿›ä»¥ä¸‹æ–‡æœ¬ï¼š\n\n${selectedText}`
        }
      ]
    })
    
    return response.content
  }
  
  async generateOutline(content: string): Promise<string[]> {
    // ç”Ÿæˆæ–‡æ¡£å¤§çº²é€»è¾‘
  }
  
  async translateText(text: string, targetLang: string): Promise<string> {
    // æ–‡æœ¬ç¿»è¯‘é€»è¾‘
  }
}
```

### 2. æ™ºèƒ½æœç´¢ä¸æ¨è

```typescript
// Client/Service/SmartSearch/search.service.ts
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

export class SmartSearchService {
  private llmService = useLlmServiceStore()
  
  async semanticSearch(query: string, documents: string[]): Promise<SearchResult[]> {
    // 1. è·å–æŸ¥è¯¢å‘é‡
    const queryEmbedding = await this.llmService.embed(query)
    
    // 2. è·å–æ–‡æ¡£å‘é‡
    const docEmbeddings = await Promise.all(
      documents.map(doc => this.llmService.embed(doc))
    )
    
    // 3. è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
    const results = this.calculateSimilarity(queryEmbedding, docEmbeddings)
    
    return results
  }
  
  async generateSearchSuggestions(partialQuery: string): Promise<string[]> {
    const model = await this.llmService.selectModel('LLM')
    if (!model) return []
    
    const response = await this.llmService.chat({
      modelId: model.id,
      messages: [
        {
          role: 'system',
          content: 'æ ¹æ®ç”¨æˆ·çš„éƒ¨åˆ†è¾“å…¥ï¼Œç”Ÿæˆ5ä¸ªç›¸å…³çš„æœç´¢å»ºè®®ã€‚'
        },
        {
          role: 'user',
          content: partialQuery
        }
      ]
    })
    
    return this.parseSearchSuggestions(response.content)
  }
}
```

---

## ğŸ”§ é…ç½®ç®¡ç†æœ€ä½³å®è·µ

### 1. æä¾›å•†é…ç½®

```typescript
// æ·»åŠ æ–°çš„ OpenAI æä¾›å•†
const newProvider = {
  name: 'openai',
  displayName: 'OpenAI GPT',
  description: 'Official OpenAI API with GPT models',
  apiKey: 'sk-your-api-key-here',
  baseUrl: 'https://api.openai.com/v1',
  status: 'active' as const,
  defaultConfig: {
    timeout: 30000,
    maxRetries: 3,
    contextLength: 8192,
    maxTokens: 4096,
    completionMode: 'å¯¹è¯' as const,
    functionCalling: 'æ”¯æŒ' as const,
    structuredOutput: 'æ”¯æŒ' as const,
    systemPromptSeparator: '\n\n'
  }
}

const settingsStore = useSettingsLlmStore()
await settingsStore.addProvider(newProvider)
```

### 2. æ¨¡å‹é€‰æ‹©é…ç½®

```typescript
// é€‰æ‹©æ´»è·ƒæ¨¡å‹
await settingsStore.toggleModelSelection('openai', 'LLM', 'gpt-4o')
await settingsStore.toggleModelSelection('openai', 'LLM', 'gpt-4')

// è®¾ç½®é¦–é€‰æ¨¡å‹
await settingsStore.setPreferredModel('openai', 'LLM', 'gpt-4o')
```

### 3. é…ç½®å¯¼å…¥å¯¼å‡º

```typescript
// å¯¼å‡ºé…ç½®
const configJson = await settingsStore.exportConfig()
localStorage.setItem('llm-backup', configJson)

// å¯¼å…¥é…ç½®
const backupConfig = localStorage.getItem('llm-backup')
if (backupConfig) {
  await settingsStore.importConfig(backupConfig)
}
```

---

## ğŸ› é”™è¯¯å¤„ç†ä¸è°ƒè¯•

### å¸¸è§é”™è¯¯ç±»å‹

| é”™è¯¯ç±»å‹ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|------|----------|
| `API_KEY_INVALID` | API å¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ | æ£€æŸ¥å¹¶æ›´æ–° API å¯†é’¥ |
| `NETWORK_ERROR` | ç½‘ç»œè¿æ¥é—®é¢˜ | æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½® |
| `RATE_LIMIT_EXCEEDED` | è¯·æ±‚é¢‘ç‡è¶…é™ | é™ä½è¯·æ±‚é¢‘ç‡æˆ–å‡çº§ API è®¡åˆ’ |
| `MODEL_NOT_FOUND` | æŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨ | åˆ·æ–°æ¨¡å‹åˆ—è¡¨æˆ–é€‰æ‹©å…¶ä»–æ¨¡å‹ |
| `INSUFFICIENT_QUOTA` | API é…é¢ä¸è¶³ | æ£€æŸ¥è´¦æˆ·ä½™é¢æˆ–ä½¿ç”¨é™åˆ¶ |

### è°ƒè¯•æŠ€å·§

```typescript
// 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
const llmService = useLlmServiceStore()
llmService.setDebugMode(true)

// 2. ç›‘å¬é”™è¯¯äº‹ä»¶
llmService.onError((error) => {
  console.error('LLM æœåŠ¡é”™è¯¯:', error)
  // å‘é€é”™è¯¯æŠ¥å‘Šæˆ–æ˜¾ç¤ºç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
})

// 3. æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
const models = llmService.getActiveModels('LLM')
console.log('å¯ç”¨æ¨¡å‹:', models.map(m => m.name))

// 4. æµ‹è¯•è¿æ¥
const testResult = await settingsStore.testProviderConnection('openai')
console.log('è¿æ¥æµ‹è¯•ç»“æœ:', testResult)
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è¯·æ±‚ä¼˜åŒ–

- **æ‰¹é‡å¤„ç†**: åˆå¹¶å¤šä¸ªå°è¯·æ±‚ä¸ºå•ä¸ªå¤§è¯·æ±‚
- **ç¼“å­˜ç­–ç•¥**: å¯¹ç›¸åŒè¾“å…¥çš„ç»“æœè¿›è¡Œç¼“å­˜
- **è¯·æ±‚å»é‡**: é¿å…åŒæ—¶å‘é€ç›¸åŒçš„è¯·æ±‚

### 2. å†…å­˜ç®¡ç†

- **æµå¼å¤„ç†**: å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆä½¿ç”¨æµå¼ API
- **åŠæ—¶æ¸…ç†**: æ¸…ç†ä¸å†ä½¿ç”¨çš„èŠå¤©å†å²å’Œç¼“å­˜
- **åˆ†é¡µåŠ è½½**: å¯¹å†å²è®°å½•è¿›è¡Œåˆ†é¡µå¤„ç†

### 3. ç”¨æˆ·ä½“éªŒ

- **åŠ è½½çŠ¶æ€**: æ˜¾ç¤ºè¯·æ±‚è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
- **é”™è¯¯é‡è¯•**: è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚
- **ç¦»çº¿å¤„ç†**: åœ¨ç½‘ç»œä¸å¯ç”¨æ—¶æä¾›åŸºæœ¬åŠŸèƒ½

---

## ğŸ”„ ç‰ˆæœ¬å†å²ä¸è·¯çº¿å›¾

### å½“å‰ç‰ˆæœ¬ (v1.0)

- âœ… å¤šæä¾›å•†é…ç½®ç®¡ç†
- âœ… æ´»è·ƒæ¨¡å‹é€‰æ‹©ä¸æŒä¹…åŒ–
- âœ… è¿æ¥æµ‹è¯•ä¸æ¨¡å‹å‘ç°
- âœ… å®Œæ•´çš„å‰ç«¯é…ç½® UI
- âœ… ç±»å‹å®‰å…¨çš„ IPC é€šä¿¡
- âœ… å®Œæ•´çš„ LLM Chat å¯¹è¯ç³»ç»Ÿ
- âœ… æµå¼å“åº”å¤„ç†
- âœ… å¯¹è¯å†å²ç®¡ç†ï¼ˆSQLite æ•°æ®åº“ï¼‰
- âœ… äº‹ä»¶é©±åŠ¨æ¶æ„
- âœ… Element Plus æ ‡ç­¾é¡µç®¡ç†
- âœ… å¯¹è¯æ•°æ®æŒä¹…åŒ–

### è®¡åˆ’ä¸­çš„åŠŸèƒ½ (v1.1+)

- [ ] å…¨å±€ LLM æœåŠ¡æ¥å£ï¼ˆä¾›å…¶ä»–ä¸šåŠ¡æ¨¡å—ä½¿ç”¨ï¼‰
- [ ] æ–‡æœ¬åµŒå…¥ API è°ƒç”¨
- [ ] æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬è·Ÿè¸ª
- [ ] è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
- [ ] å¤šæ¨¡æ€æ”¯æŒï¼ˆå›¾åƒã€éŸ³é¢‘ï¼‰
- [ ] æ’ä»¶åŒ–çš„æ¨¡å‹åå¤„ç†
- [ ] å¯¹è¯å¯¼å‡ºåŠŸèƒ½
- [ ] å¯¹è¯æœç´¢å’Œæ ‡ç­¾åˆ†ç±»

---

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [æ¶æ„è®¾è®¡æ€»è§ˆ](./æ¶æ„è®¾è®¡æ€»è§ˆ.md)
- [å¤šçª—å£ç³»ç»Ÿè®¾è®¡æ–‡æ¡£](./å¤šçª—å£ç³»ç»Ÿè®¾è®¡æ–‡æ¡£.md)
- [æ–‡ä»¶ç³»ç»Ÿä¸é¡¹ç›®ç»“æ„è®¾è®¡æ–‡æ¡£](./æ–‡ä»¶ç³»ç»Ÿä¸é¡¹ç›®ç»“æ„è®¾è®¡æ–‡æ¡£.md)
- [å‘½ä»¤é¢æ¿ä¸å³æ -å¿«é€Ÿå‚è€ƒ](./å‘½ä»¤é¢æ¿ä¸å³æ -å¿«é€Ÿå‚è€ƒ.md)

---

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ16æ—¥  
**è´Ÿè´£äºº**: Nimbria å¼€å‘å›¢é˜Ÿ

### æ›´æ–°æ—¥å¿—

**v1.0 (2025-10-16)**:
- âœ… æ–°å¢å®Œæ•´çš„ LLM Chat å¯¹è¯ç³»ç»Ÿå®ç°
- âœ… æ–°å¢äº‹ä»¶é©±åŠ¨æ¶æ„è¯´æ˜
- âœ… æ–°å¢æ•°æ®åº“æŒä¹…åŒ–æ–¹æ¡ˆ
- âœ… æ–°å¢æµå¼å“åº”å¤„ç†æœºåˆ¶
- âœ… æ–°å¢ Element Plus æ ‡ç­¾é¡µç®¡ç†
- âœ… æ›´æ–° IPC é€šä¿¡åè®®ï¼ˆå¯¹è¯ç®¡ç†å’Œäº‹ä»¶å¹¿æ’­ï¼‰
- âœ… æ–°å¢ LLM Chat ä½¿ç”¨æŒ‡å—å’Œç¤ºä¾‹ä»£ç 
