# Nimbria AI æ¨¡å‹æœåŠ¡ä¸è°ƒç”¨ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2025å¹´10æœˆ14æ—¥  
**æ–‡æ¡£çŠ¶æ€**: åæ˜ å®é™…å®ç°  

---

## ğŸ“‹ ç³»ç»Ÿæ¦‚è¿°

Nimbria çš„ AI æ¨¡å‹æœåŠ¡ä¸è°ƒç”¨ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ã€ç±»å‹å®‰å…¨çš„å¤§è¯­è¨€æ¨¡å‹äº¤äº’å¹³å°ã€‚ç³»ç»Ÿæ”¯æŒå¤šæä¾›å•†ç®¡ç†ã€æ´»è·ƒæ¨¡å‹é…ç½®ã€å®æ—¶å¯¹è¯äº¤äº’ï¼Œå¹¶ä¸ºå…¶ä»–ä¸šåŠ¡æ¨¡å—æä¾›ç®€æ´çš„ API æ¥å£ã€‚é€šè¿‡ Electron ä¸»è¿›ç¨‹çš„å®‰å…¨éš”ç¦»ï¼Œç¡®ä¿ API å¯†é’¥å’Œæ¨¡å‹è°ƒç”¨çš„å®‰å…¨æ€§ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **å¤šæä¾›å•†æ”¯æŒ**: ç»Ÿä¸€ç®¡ç† OpenAIã€Anthropicã€è‡ªå®šä¹‰æä¾›å•†ç­‰å¤šç§ LLM æœåŠ¡
- **æ´»è·ƒæ¨¡å‹é…ç½®**: ç”¨æˆ·å¯é€‰æ‹©å’Œé…ç½®ä¸åŒç±»å‹çš„æ´»è·ƒæ¨¡å‹ï¼ˆLLMã€æ–‡æœ¬åµŒå…¥ã€å›¾åƒç”Ÿæˆç­‰ï¼‰
- **é…ç½®æŒä¹…åŒ–**: æ‰€æœ‰é…ç½®è‡ªåŠ¨ä¿å­˜åˆ° YAML æ–‡ä»¶ï¼Œæ”¯æŒå¯¼å…¥å¯¼å‡º
- **ç±»å‹å®‰å…¨è°ƒç”¨**: å®Œæ•´çš„ TypeScript æ”¯æŒï¼Œç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
- **å…¨å±€æœåŠ¡æ¥å£**: ä¸ºå…¶ä»–ä¸šåŠ¡æ¨¡å—æä¾›ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨ API
- **è¿æ¥æµ‹è¯•ä¸æ¨¡å‹å‘ç°**: è‡ªåŠ¨æµ‹è¯•è¿æ¥å¹¶å‘ç°å¯ç”¨æ¨¡å‹
- **é”™è¯¯å¤„ç†ä¸é‡è¯•**: å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶å’Œè‡ªåŠ¨é‡è¯•é€»è¾‘

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ•°æ®é“¾è·¯

```mermaid
graph TD
    A[ä¸šåŠ¡æ¨¡å—<br/>å¦‚ Markdown ç¼–è¾‘å™¨] -->|è°ƒç”¨å…¨å±€ LLM API| B[LLM Service Store<br/>å‰ç«¯ç»Ÿä¸€æ¥å£]
    B -->|IPC è°ƒç”¨| C[ä¸»è¿›ç¨‹ LLM Handlers]
    C --> D{æ“ä½œç±»å‹}
    D -->|é…ç½®ç®¡ç†| E[LlmConfigManager<br/>YAML é…ç½®æ–‡ä»¶]
    D -->|æ¨¡å‹è°ƒç”¨| F[LlmChatService<br/>å®é™… API è°ƒç”¨]
    F -->|æ ¹æ®æ¨¡å‹IDè·¯ç”±| G[LlmApiClient<br/>OpenAI SDK å°è£…]
    G -->|HTTP è¯·æ±‚| H[å¤–éƒ¨ LLM æä¾›å•†<br/>OpenAI/Anthropic/Custom]
    E -->|è¯»å†™é…ç½®| I[providers.yaml<br/>ç”¨æˆ·æ•°æ®ç›®å½•]
    
    subgraph Frontend[å‰ç«¯æ¸²æŸ“è¿›ç¨‹]
        A
        B
        J[Settings UI<br/>æ¨¡å‹æœåŠ¡é…ç½®é¡µé¢]
    end
    
    subgraph MainProcess[ä¸»è¿›ç¨‹ Electron]
        C
        E
        F
        G
    end
    
    J -->|é…ç½®ç®¡ç†| B
    H -->|å“åº”| G
```

### æ–‡ä»¶æ¶æ„

```
Nimbria/
â”œâ”€â”€ Client/                                    # å‰ç«¯ä»£ç 
â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â”œâ”€â”€ settings/                          # é…ç½®ç®¡ç† Store
â”‚   â”‚   â”‚   â”œâ”€â”€ settings.llm.store.ts          # LLM é…ç½®çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ DataSource.ts                  # æ•°æ®æºæŠ½è±¡å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts                       # å‰ç«¯ç±»å‹å®šä¹‰
â”‚   â”‚   â”‚   â””â”€â”€ llm.mock.ts                    # Mock æ•°æ®ï¼ˆå·²åºŸå¼ƒï¼‰
â”‚   â”‚   â””â”€â”€ llm/                               # å…¨å±€ LLM æœåŠ¡ Store
â”‚   â”‚       â”œâ”€â”€ llm.service.store.ts           # å…¨å±€ LLM è°ƒç”¨æ¥å£
â”‚   â”‚       â””â”€â”€ types.ts                       # æœåŠ¡å±‚ç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ GUI/components/HomeDashboardPage/Settings/  # é…ç½® UI ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ Settings.LlmConfig.vue             # ä¸»é…ç½®é¡µé¢
â”‚   â”‚   â”œâ”€â”€ Settings.LlmConfig.ProviderList.vue    # æä¾›å•†åˆ—è¡¨
â”‚   â”‚   â”œâ”€â”€ Settings.LlmConfig.ActiveModels.vue    # æ´»è·ƒæ¨¡å‹ç®¡ç†
â”‚   â”‚   â””â”€â”€ Settings.LlmConfig.*.vue           # å…¶ä»–é…ç½®ç»„ä»¶
â”‚   â””â”€â”€ types/core/window.d.ts                 # å…¨å±€ API ç±»å‹å®šä¹‰
â”œâ”€â”€ src-electron/                              # ä¸»è¿›ç¨‹ä»£ç 
â”‚   â”œâ”€â”€ services/llm-service/                  # LLM æœåŠ¡å±‚
â”‚   â”‚   â”œâ”€â”€ llm-config-manager.ts              # é…ç½®ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ llm-api-client.ts                  # API å®¢æˆ·ç«¯å°è£…
â”‚   â”‚   â”œâ”€â”€ llm-chat-service.ts                # èŠå¤©æœåŠ¡ï¼ˆå¾…å®ç°ï¼‰
â”‚   â”‚   â””â”€â”€ types.ts                           # åç«¯ç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ ipc/main-renderer/
â”‚   â”‚   â””â”€â”€ llm-handlers.ts                    # IPC å¤„ç†å™¨
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ main-preload.ts                    # API æš´éœ²
â”‚       â””â”€â”€ app-manager.ts                     # æœåŠ¡æ³¨å†Œ
â””â”€â”€ AppData/llm-config/                        # é…ç½®æ–‡ä»¶å­˜å‚¨
    â””â”€â”€ providers.yaml                         # æä¾›å•†é…ç½®æ–‡ä»¶
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. LlmConfigManager (é…ç½®ç®¡ç†)

**èŒè´£**: ç®¡ç†æä¾›å•†é…ç½®çš„ YAML æ–‡ä»¶è¯»å†™å’ŒæŒä¹…åŒ–

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class LlmConfigManager {
  // é…ç½®æ–‡ä»¶ç®¡ç†
  async loadProviders(): Promise<ModelProvider[]>
  async saveProviders(providers: ModelProvider[]): Promise<void>
  
  // æä¾›å•†ç®¡ç†
  async addProvider(provider: ModelProvider): Promise<ModelProvider>
  async updateProvider(providerId: string, updates: Partial<ModelProvider>): Promise<ModelProvider>
  async removeProvider(providerId: string): Promise<void>
  async getProvider(providerId: string): Promise<ModelProvider | null>
}
```

**é…ç½®æ–‡ä»¶ç»“æ„**:
```yaml
providers:
  - id: openai
    name: openai
    displayName: OpenAI
    status: active
    apiKey: sk-xxxxx
    baseUrl: https://api.openai.com/v1
    activeModels:
      LLM:
        selectedModels: [gpt-4o, gpt-4]
        preferredModel: gpt-4o
      TEXT_EMBEDDING:
        selectedModels: [text-embedding-3-large]
        preferredModel: text-embedding-3-large
```

### 2. LlmApiClient (API å®¢æˆ·ç«¯)

**èŒè´£**: å°è£… OpenAI SDKï¼Œæä¾›è¿æ¥æµ‹è¯•å’Œæ¨¡å‹å‘ç°åŠŸèƒ½

**æ ¸å¿ƒæ–¹æ³•**:
```typescript
class LlmApiClient {
  // è¿æ¥æµ‹è¯•
  async testConnection(): Promise<ConnectionTestResult>
  
  // æ¨¡å‹å‘ç°
  async discoverModels(): Promise<DiscoveredModel[]>
  
  // å®é™…è°ƒç”¨ï¼ˆå¾…æ‰©å±•ï¼‰
  async chat(messages: ChatMessage[]): Promise<ChatResponse>
  async embed(text: string): Promise<EmbeddingResponse>
}
```

### 3. Settings LLM Store (å‰ç«¯çŠ¶æ€ç®¡ç†)

**èŒè´£**: ç®¡ç†å‰ç«¯çš„ LLM é…ç½®çŠ¶æ€ï¼Œæä¾›å“åº”å¼æ•°æ®å’Œæ“ä½œæ–¹æ³•

**æ ¸å¿ƒçŠ¶æ€**:
```typescript
export const useSettingsLlmStore = defineStore('settings-llm', () => {
  // åŸºç¡€çŠ¶æ€
  const providers = ref<ModelProvider[]>([])
  const loading = ref(false)
  const error = ref<string | null>(null)
  
  // è®¡ç®—å±æ€§
  const activeProviders = computed(() => providers.value.filter(p => p.status === 'active'))
  const activeModelTypes = computed(() => /* è·å–æ‰€æœ‰æ´»è·ƒæ¨¡å‹ç±»å‹ */)
  
  // æ“ä½œæ–¹æ³•
  async function initialize(): Promise<void>
  async function addProvider(provider: Omit<ModelProvider, 'id'>): Promise<boolean>
  async function toggleModelSelection(providerId: string, modelType: string, modelName: string): Promise<boolean>
  async function setPreferredModel(providerId: string, modelType: string, modelName: string): Promise<boolean>
})
```

### 4. å…¨å±€ LLM æœåŠ¡ Store (å¾…å®ç°)

**èŒè´£**: ä¸ºå…¶ä»–ä¸šåŠ¡æ¨¡å—æä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£

```typescript
export const useLlmServiceStore = defineStore('llm-service', () => {
  // è·å–å¯ç”¨æ¨¡å‹
  function getActiveModels(modelType: ModelType): ActiveModel[]
  function getPreferredModel(modelType: ModelType): ActiveModel | null
  
  // æ¨¡å‹è°ƒç”¨
  async function chat(options: ChatOptions): Promise<ChatResponse>
  async function chatStream(options: ChatStreamOptions): Promise<ReadableStream>
  async function embed(text: string, modelId?: string): Promise<EmbeddingResponse>
  
  // æ¨¡å‹é€‰æ‹©
  async function selectModel(modelType: ModelType): Promise<ActiveModel | null>
})
```

---

## ğŸ”— IPC é€šä¿¡åè®®

### é…ç½®ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm:get-providers` | `undefined` | `{ success: boolean; providers?: ModelProvider[] }` | è·å–æ‰€æœ‰æä¾›å•† |
| `llm:add-provider` | `{ provider: Omit<ModelProvider, 'id'> }` | `{ success: boolean; provider?: ModelProvider }` | æ·»åŠ æ–°æä¾›å•† |
| `llm:remove-provider` | `{ providerId: string }` | `{ success: boolean }` | åˆ é™¤æä¾›å•† |
| `llm:activate-provider` | `{ providerId: string }` | `{ success: boolean; provider?: ModelProvider }` | æ¿€æ´»æä¾›å•† |
| `llm:refresh-models` | `{ providerId: string }` | `{ success: boolean; modelsCount?: number }` | åˆ·æ–°æ¨¡å‹åˆ—è¡¨ |
| `llm:test-connection` | `{ providerId: string }` | `{ success: boolean; message?: string }` | æµ‹è¯•è¿æ¥ |

### æ´»è·ƒæ¨¡å‹ç®¡ç† IPC

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm:toggle-model-selection` | `{ providerId: string; modelType: string; modelName: string }` | `{ success: boolean }` | åˆ‡æ¢æ¨¡å‹é€‰æ‹©çŠ¶æ€ |
| `llm:set-preferred-model` | `{ providerId: string; modelType: string; modelName: string }` | `{ success: boolean }` | è®¾ç½®é¦–é€‰æ¨¡å‹ |

### æ¨¡å‹è°ƒç”¨ IPC (å¾…å®ç°)

| é€šé“å | è¯·æ±‚ç±»å‹ | å“åº”ç±»å‹ | ç”¨é€” |
|-------|---------|----------|------|
| `llm:get-active-models` | `{ modelType: ModelType }` | `{ success: boolean; models?: ActiveModel[] }` | è·å–æ´»è·ƒæ¨¡å‹åˆ—è¡¨ |
| `llm:chat` | `{ modelId: string; messages: ChatMessage[] }` | `{ success: boolean; response?: ChatResponse }` | å‘é€èŠå¤©æ¶ˆæ¯ |
| `llm:chat-stream` | `{ modelId: string; messages: ChatMessage[] }` | `{ success: boolean; streamId?: string }` | æµå¼èŠå¤© |

---

## ğŸ“Š çŠ¶æ€ç®¡ç†æ¶æ„

### æ•°æ®æµå‘

```mermaid
graph LR
    A[ç”¨æˆ·æ“ä½œ] --> B[Settings UI ç»„ä»¶]
    B --> C[Settings LLM Store]
    C --> D[DataSource å±‚]
    D --> E[IPC è°ƒç”¨]
    E --> F[ä¸»è¿›ç¨‹ Handlers]
    F --> G[LlmConfigManager]
    G --> H[YAML é…ç½®æ–‡ä»¶]
    
    I[ä¸šåŠ¡æ¨¡å—] --> J[LLM Service Store]
    J --> K[IPC è°ƒç”¨]
    K --> L[LlmChatService]
    L --> M[LlmApiClient]
    M --> N[å¤–éƒ¨ API]
```

### çŠ¶æ€åŒæ­¥æœºåˆ¶

1. **é…ç½®å˜æ›´**: ç”¨æˆ·åœ¨è®¾ç½®é¡µé¢çš„æ“ä½œä¼šç«‹å³åŒæ­¥åˆ°ä¸»è¿›ç¨‹å¹¶æŒä¹…åŒ–
2. **è·¨çª—å£åŒæ­¥**: é…ç½®å˜æ›´ä¼šé€šè¿‡ IPC å¹¿æ’­åˆ°æ‰€æœ‰çª—å£
3. **é”™è¯¯å¤„ç†**: æ“ä½œå¤±è´¥æ—¶å‰ç«¯çŠ¶æ€ä¼šå›æ»šï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§

---

## ğŸš€ ä¸šåŠ¡æ¨¡å—è°ƒç”¨æŒ‡å—

### å¿«é€Ÿä¸Šæ‰‹ - è·å–æ´»è·ƒæ¨¡å‹

```typescript
// åœ¨ä»»ä½• Vue ç»„ä»¶æˆ–ä¸šåŠ¡é€»è¾‘ä¸­
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

export default {
  setup() {
    const llmService = useLlmServiceStore()
    
    // è·å–å¯ç”¨çš„ LLM æ¨¡å‹
    const availableModels = llmService.getActiveModels('LLM')
    
    // è·å–é¦–é€‰æ¨¡å‹
    const preferredModel = llmService.getPreferredModel('LLM')
    
    return { availableModels, preferredModel }
  }
}
```

### å‘èµ·èŠå¤©å¯¹è¯

```typescript
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

async function startChat() {
  const llmService = useLlmServiceStore()
  
  // è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹ï¼ˆå¦‚æœæœ‰å¤šä¸ªå¯ç”¨ï¼‰
  const selectedModel = await llmService.selectModel('LLM')
  if (!selectedModel) {
    console.warn('æ²¡æœ‰å¯ç”¨çš„ LLM æ¨¡å‹')
    return
  }
  
  // å‘é€æ¶ˆæ¯
  try {
    const response = await llmService.chat({
      modelId: selectedModel.id,
      messages: [
        { role: 'user', content: 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±' }
      ],
      options: {
        temperature: 0.7,
        maxTokens: 1000
      }
    })
    
    console.log('AI å›å¤:', response.content)
  } catch (error) {
    console.error('èŠå¤©å¤±è´¥:', error)
  }
}
```

### æµå¼å¯¹è¯

```typescript
async function streamChat() {
  const llmService = useLlmServiceStore()
  const selectedModel = await llmService.selectModel('LLM')
  
  if (!selectedModel) return
  
  try {
    const stream = await llmService.chatStream({
      modelId: selectedModel.id,
      messages: [
        { role: 'user', content: 'è¯·å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„è¯—' }
      ]
    })
    
    const reader = stream.getReader()
    let result = ''
    
    while (true) {
      const { done, value } = await reader.read()
      if (done) break
      
      result += value
      console.log('å®æ—¶å†…å®¹:', result)
    }
  } catch (error) {
    console.error('æµå¼èŠå¤©å¤±è´¥:', error)
  }
}
```

### æ–‡æœ¬åµŒå…¥

```typescript
async function getEmbedding() {
  const llmService = useLlmServiceStore()
  
  try {
    const embedding = await llmService.embed(
      'è¿™æ˜¯éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬å†…å®¹',
      'text-embedding-3-large' // å¯é€‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é¦–é€‰æ¨¡å‹
    )
    
    console.log('å‘é‡ç»´åº¦:', embedding.dimensions)
    console.log('å‘é‡æ•°æ®:', embedding.vector)
  } catch (error) {
    console.error('åµŒå…¥å¤±è´¥:', error)
  }
}
```

---

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. Markdown ç¼–è¾‘å™¨ AI åŠ©æ‰‹

```typescript
// Client/stores/projectPage/Markdown/ai-assistant.ts
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

export class MarkdownAiAssistant {
  private llmService = useLlmServiceStore()
  
  async improveText(selectedText: string): Promise<string> {
    const model = await this.llmService.selectModel('LLM')
    if (!model) throw new Error('æ²¡æœ‰å¯ç”¨çš„ LLM æ¨¡å‹')
    
    const response = await this.llmService.chat({
      modelId: model.id,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘åŠ©æ‰‹ï¼Œè¯·æ”¹è¿›ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°ã€å‡†ç¡®ã€æ˜“è¯»ã€‚'
        },
        {
          role: 'user',
          content: `è¯·æ”¹è¿›ä»¥ä¸‹æ–‡æœ¬ï¼š\n\n${selectedText}`
        }
      ]
    })
    
    return response.content
  }
  
  async generateOutline(content: string): Promise<string[]> {
    // ç”Ÿæˆæ–‡æ¡£å¤§çº²é€»è¾‘
  }
  
  async translateText(text: string, targetLang: string): Promise<string> {
    // æ–‡æœ¬ç¿»è¯‘é€»è¾‘
  }
}
```

### 2. æ™ºèƒ½æœç´¢ä¸æ¨è

```typescript
// Client/Service/SmartSearch/search.service.ts
import { useLlmServiceStore } from '@stores/llm/llm.service.store'

export class SmartSearchService {
  private llmService = useLlmServiceStore()
  
  async semanticSearch(query: string, documents: string[]): Promise<SearchResult[]> {
    // 1. è·å–æŸ¥è¯¢å‘é‡
    const queryEmbedding = await this.llmService.embed(query)
    
    // 2. è·å–æ–‡æ¡£å‘é‡
    const docEmbeddings = await Promise.all(
      documents.map(doc => this.llmService.embed(doc))
    )
    
    // 3. è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
    const results = this.calculateSimilarity(queryEmbedding, docEmbeddings)
    
    return results
  }
  
  async generateSearchSuggestions(partialQuery: string): Promise<string[]> {
    const model = await this.llmService.selectModel('LLM')
    if (!model) return []
    
    const response = await this.llmService.chat({
      modelId: model.id,
      messages: [
        {
          role: 'system',
          content: 'æ ¹æ®ç”¨æˆ·çš„éƒ¨åˆ†è¾“å…¥ï¼Œç”Ÿæˆ5ä¸ªç›¸å…³çš„æœç´¢å»ºè®®ã€‚'
        },
        {
          role: 'user',
          content: partialQuery
        }
      ]
    })
    
    return this.parseSearchSuggestions(response.content)
  }
}
```

---

## ğŸ”§ é…ç½®ç®¡ç†æœ€ä½³å®è·µ

### 1. æä¾›å•†é…ç½®

```typescript
// æ·»åŠ æ–°çš„ OpenAI æä¾›å•†
const newProvider = {
  name: 'openai',
  displayName: 'OpenAI GPT',
  description: 'Official OpenAI API with GPT models',
  apiKey: 'sk-your-api-key-here',
  baseUrl: 'https://api.openai.com/v1',
  status: 'active' as const,
  defaultConfig: {
    timeout: 30000,
    maxRetries: 3,
    contextLength: 8192,
    maxTokens: 4096,
    completionMode: 'å¯¹è¯' as const,
    functionCalling: 'æ”¯æŒ' as const,
    structuredOutput: 'æ”¯æŒ' as const,
    systemPromptSeparator: '\n\n'
  }
}

const settingsStore = useSettingsLlmStore()
await settingsStore.addProvider(newProvider)
```

### 2. æ¨¡å‹é€‰æ‹©é…ç½®

```typescript
// é€‰æ‹©æ´»è·ƒæ¨¡å‹
await settingsStore.toggleModelSelection('openai', 'LLM', 'gpt-4o')
await settingsStore.toggleModelSelection('openai', 'LLM', 'gpt-4')

// è®¾ç½®é¦–é€‰æ¨¡å‹
await settingsStore.setPreferredModel('openai', 'LLM', 'gpt-4o')
```

### 3. é…ç½®å¯¼å…¥å¯¼å‡º

```typescript
// å¯¼å‡ºé…ç½®
const configJson = await settingsStore.exportConfig()
localStorage.setItem('llm-backup', configJson)

// å¯¼å…¥é…ç½®
const backupConfig = localStorage.getItem('llm-backup')
if (backupConfig) {
  await settingsStore.importConfig(backupConfig)
}
```

---

## ğŸ› é”™è¯¯å¤„ç†ä¸è°ƒè¯•

### å¸¸è§é”™è¯¯ç±»å‹

| é”™è¯¯ç±»å‹ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|------|----------|
| `API_KEY_INVALID` | API å¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ | æ£€æŸ¥å¹¶æ›´æ–° API å¯†é’¥ |
| `NETWORK_ERROR` | ç½‘ç»œè¿æ¥é—®é¢˜ | æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½® |
| `RATE_LIMIT_EXCEEDED` | è¯·æ±‚é¢‘ç‡è¶…é™ | é™ä½è¯·æ±‚é¢‘ç‡æˆ–å‡çº§ API è®¡åˆ’ |
| `MODEL_NOT_FOUND` | æŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨ | åˆ·æ–°æ¨¡å‹åˆ—è¡¨æˆ–é€‰æ‹©å…¶ä»–æ¨¡å‹ |
| `INSUFFICIENT_QUOTA` | API é…é¢ä¸è¶³ | æ£€æŸ¥è´¦æˆ·ä½™é¢æˆ–ä½¿ç”¨é™åˆ¶ |

### è°ƒè¯•æŠ€å·§

```typescript
// 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
const llmService = useLlmServiceStore()
llmService.setDebugMode(true)

// 2. ç›‘å¬é”™è¯¯äº‹ä»¶
llmService.onError((error) => {
  console.error('LLM æœåŠ¡é”™è¯¯:', error)
  // å‘é€é”™è¯¯æŠ¥å‘Šæˆ–æ˜¾ç¤ºç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
})

// 3. æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
const models = llmService.getActiveModels('LLM')
console.log('å¯ç”¨æ¨¡å‹:', models.map(m => m.name))

// 4. æµ‹è¯•è¿æ¥
const testResult = await settingsStore.testProviderConnection('openai')
console.log('è¿æ¥æµ‹è¯•ç»“æœ:', testResult)
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è¯·æ±‚ä¼˜åŒ–

- **æ‰¹é‡å¤„ç†**: åˆå¹¶å¤šä¸ªå°è¯·æ±‚ä¸ºå•ä¸ªå¤§è¯·æ±‚
- **ç¼“å­˜ç­–ç•¥**: å¯¹ç›¸åŒè¾“å…¥çš„ç»“æœè¿›è¡Œç¼“å­˜
- **è¯·æ±‚å»é‡**: é¿å…åŒæ—¶å‘é€ç›¸åŒçš„è¯·æ±‚

### 2. å†…å­˜ç®¡ç†

- **æµå¼å¤„ç†**: å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆä½¿ç”¨æµå¼ API
- **åŠæ—¶æ¸…ç†**: æ¸…ç†ä¸å†ä½¿ç”¨çš„èŠå¤©å†å²å’Œç¼“å­˜
- **åˆ†é¡µåŠ è½½**: å¯¹å†å²è®°å½•è¿›è¡Œåˆ†é¡µå¤„ç†

### 3. ç”¨æˆ·ä½“éªŒ

- **åŠ è½½çŠ¶æ€**: æ˜¾ç¤ºè¯·æ±‚è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
- **é”™è¯¯é‡è¯•**: è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚
- **ç¦»çº¿å¤„ç†**: åœ¨ç½‘ç»œä¸å¯ç”¨æ—¶æä¾›åŸºæœ¬åŠŸèƒ½

---

## ğŸ”„ ç‰ˆæœ¬å†å²ä¸è·¯çº¿å›¾

### å½“å‰ç‰ˆæœ¬ (v1.0)

- âœ… å¤šæä¾›å•†é…ç½®ç®¡ç†
- âœ… æ´»è·ƒæ¨¡å‹é€‰æ‹©ä¸æŒä¹…åŒ–
- âœ… è¿æ¥æµ‹è¯•ä¸æ¨¡å‹å‘ç°
- âœ… å®Œæ•´çš„å‰ç«¯é…ç½® UI
- âœ… ç±»å‹å®‰å…¨çš„ IPC é€šä¿¡

### è®¡åˆ’ä¸­çš„åŠŸèƒ½ (v1.1+)

- [ ] å®é™…çš„èŠå¤©å’ŒåµŒå…¥ API è°ƒç”¨
- [ ] æµå¼å“åº”å¤„ç†
- [ ] èŠå¤©å†å²ç®¡ç†
- [ ] æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬è·Ÿè¸ª
- [ ] è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
- [ ] å¤šæ¨¡æ€æ”¯æŒï¼ˆå›¾åƒã€éŸ³é¢‘ï¼‰
- [ ] æ’ä»¶åŒ–çš„æ¨¡å‹åå¤„ç†

---

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [æ¶æ„è®¾è®¡æ€»è§ˆ](./æ¶æ„è®¾è®¡æ€»è§ˆ.md)
- [å¤šçª—å£ç³»ç»Ÿè®¾è®¡æ–‡æ¡£](./å¤šçª—å£ç³»ç»Ÿè®¾è®¡æ–‡æ¡£.md)
- [æ–‡ä»¶ç³»ç»Ÿä¸é¡¹ç›®ç»“æ„è®¾è®¡æ–‡æ¡£](./æ–‡ä»¶ç³»ç»Ÿä¸é¡¹ç›®ç»“æ„è®¾è®¡æ–‡æ¡£.md)
- [å‘½ä»¤é¢æ¿ä¸å³æ -å¿«é€Ÿå‚è€ƒ](./å‘½ä»¤é¢æ¿ä¸å³æ -å¿«é€Ÿå‚è€ƒ.md)

---

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ14æ—¥  
**è´Ÿè´£äºº**: Nimbria å¼€å‘å›¢é˜Ÿ
